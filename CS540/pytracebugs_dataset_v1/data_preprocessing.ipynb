{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataprocessing for getting only relavant data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing on train data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             after_merge  \\\n",
      "1790   def plot(result_pickle_file_path, show, plot_s...   \n",
      "2433       def stream_logs(self):\\n        \"\"\"Stream ...   \n",
      "26618      def addRecentProjectFile(self, projectFile...   \n",
      "26622      def addSfmAugmentation(self, withMVS=False...   \n",
      "28217      def load_pymathics_doc(self):\\n        if ...   \n",
      "\n",
      "                                            before_merge  \\\n",
      "1790   def plot(result_dict_file, show, plot_save_fil...   \n",
      "2433       def stream_logs(self):\\n        \"\"\"Stream ...   \n",
      "26618      def addRecentProjectFile(self, projectFile...   \n",
      "26622      def addSfmAugmentation(self, withMVS=False...   \n",
      "28217      def load_pymathics_doc(self):\\n        if ...   \n",
      "\n",
      "                                               filename  \\\n",
      "1790   rqalpha/mod/rqalpha_mod_sys_analyser/__init__.py   \n",
      "2433                                 binderhub/build.py   \n",
      "26618                                meshroom/ui/app.py   \n",
      "26622                     meshroom/ui/reconstruction.py   \n",
      "28217                                mathics/doc/doc.py   \n",
      "\n",
      "                              full_file_code_after_merge  \\\n",
      "1790   # -*- coding: utf-8 -*-\\n#\\n# Copyright 2017 R...   \n",
      "2433   \"\"\"\\nContains build of a docker image from a g...   \n",
      "26618  import logging\\nimport os\\nimport argparse\\n\\n...   \n",
      "26622  import logging\\nimport os\\nfrom threading impo...   \n",
      "28217  #!/usr/bin/env python3\\n# -*- coding: utf-8 -*...   \n",
      "\n",
      "                             full_file_code_before_merge  \\\n",
      "1790   # -*- coding: utf-8 -*-\\n#\\n# Copyright 2017 R...   \n",
      "2433   \"\"\"\\nContains build of a docker image from a g...   \n",
      "26618  import logging\\nimport os\\nimport argparse\\n\\n...   \n",
      "26622  import logging\\nimport os\\nfrom threading impo...   \n",
      "28217  #!/usr/bin/env python3\\n# -*- coding: utf-8 -*...   \n",
      "\n",
      "                                     function_name  \\\n",
      "1790                                          plot   \n",
      "2433                             Build.stream_logs   \n",
      "26618             MeshroomApp.addRecentProjectFile   \n",
      "26622            Reconstruction.addSfmAugmentation   \n",
      "28217  MathicsMainDocumentation.load_pymathics_doc   \n",
      "\n",
      "                                                     url  \\\n",
      "1790     https://github.com/ricequant/rqalpha/issues/109   \n",
      "2433   https://github.com/jupyterhub/binderhub/issues...   \n",
      "26618  https://github.com/alicevision/meshroom/issues...   \n",
      "26622  https://github.com/alicevision/meshroom/issues...   \n",
      "28217      https://github.com/mathics/Mathics/issues/906   \n",
      "\n",
      "                                  source code and errors  \\\n",
      "1790   [{'piece_type': 'other', 'piece_content': 'rqa...   \n",
      "2433   [{'piece_type': 'error message', 'piece_conten...   \n",
      "26618  [{'piece_type': 'error message', 'piece_conten...   \n",
      "26622  [{'piece_type': 'error message', 'piece_conten...   \n",
      "28217  [{'piece_type': 'error message', 'piece_conten...   \n",
      "\n",
      "                                          full_traceback     traceback_type  \\\n",
      "1790   Traceback (most recent call last):\\nFile \"c:\\p...          TypeError   \n",
      "2433   / # jupyter-repo2docker https://github.com/yuv...  FileNotFoundError   \n",
      "26618  [2020-05-23 16:12:48,660][ERROR] Traceback (mo...            OSError   \n",
      "26622  Traceback (most recent call last):\\nFile \"C:\\U...       RuntimeError   \n",
      "28217  $ mathicsserver\\nwarning: database file /home/...           KeyError   \n",
      "\n",
      "                         before_merge_without_docstrings  \\\n",
      "1790   def plot(result_dict_file, show, plot_save_fil...   \n",
      "2433       def stream_logs(self):\\n        \\n        ...   \n",
      "26618      def addRecentProjectFile(self, projectFile...   \n",
      "26622      def addSfmAugmentation(self, withMVS=False...   \n",
      "28217      def load_pymathics_doc(self):\\n        if ...   \n",
      "\n",
      "                          after_merge_without_docstrings  \\\n",
      "1790   def plot(result_pickle_file_path, show, plot_s...   \n",
      "2433       def stream_logs(self):\\n        \\n        ...   \n",
      "26618      def addRecentProjectFile(self, projectFile...   \n",
      "26622      def addSfmAugmentation(self, withMVS=False...   \n",
      "28217      def load_pymathics_doc(self):\\n        if ...   \n",
      "\n",
      "                      before_merge_docstrings  \\\n",
      "1790   [[sys_analyser] draw result DataFrame]   \n",
      "2433                                       []   \n",
      "26618                                      []   \n",
      "26622                                      []   \n",
      "28217                                      []   \n",
      "\n",
      "                       after_merge_docstrings  \\\n",
      "1790   [[sys_analyser] draw result DataFrame]   \n",
      "2433                                       []   \n",
      "26618                                      []   \n",
      "26622                                      []   \n",
      "28217                                      []   \n",
      "\n",
      "                            path_to_snippet_before_merge  \\\n",
      "1790   buggy_snippets_files/e93817735d3042d739fe86677...   \n",
      "2433   buggy_snippets_files/8241189c4267b81254c9ed07a...   \n",
      "26618  buggy_snippets_files/faddf4c059bd32cc1cad1a1ea...   \n",
      "26622  buggy_snippets_files/dffb9602005cbea45f7d0c6d2...   \n",
      "28217  buggy_snippets_files/c98cf1a03e1d7e716b228fbe8...   \n",
      "\n",
      "                             path_to_snippet_after_merge  \n",
      "1790   buggy_snippets_files/e93817735d3042d739fe86677...  \n",
      "2433   buggy_snippets_files/8241189c4267b81254c9ed07a...  \n",
      "26618  buggy_snippets_files/faddf4c059bd32cc1cad1a1ea...  \n",
      "26622  buggy_snippets_files/dffb9602005cbea45f7d0c6d2...  \n",
      "28217  buggy_snippets_files/c98cf1a03e1d7e716b228fbe8...  \n",
      "                                              after_merge  \\\n",
      "count                                               14118   \n",
      "unique                                              14055   \n",
      "top         def search_novel(self, query):\\n        qu...   \n",
      "freq                                                    4   \n",
      "\n",
      "                                             before_merge         filename  \\\n",
      "count                                               14118            14118   \n",
      "unique                                              14118             5352   \n",
      "top     def plot(result_dict_file, show, plot_save_fil...  mypy/semanal.py   \n",
      "freq                                                    1               63   \n",
      "\n",
      "                               full_file_code_after_merge  \\\n",
      "count                                               14118   \n",
      "unique                                               8074   \n",
      "top     #!/usr/bin/env python\\n# -*- coding: utf-8 -*-...   \n",
      "freq                                                   47   \n",
      "\n",
      "                              full_file_code_before_merge function_name  \\\n",
      "count                                               14118         14118   \n",
      "unique                                               8070         12155   \n",
      "top     #!/usr/bin/env python\\n# -*- coding: utf-8 -*-...          main   \n",
      "freq                                                   47           125   \n",
      "\n",
      "                                               url  \\\n",
      "count                                        14118   \n",
      "unique                                        4693   \n",
      "top     https://github.com/numba/numba/issues/4944   \n",
      "freq                                           231   \n",
      "\n",
      "                                   source code and errors  \\\n",
      "count                                               14118   \n",
      "unique                                               4693   \n",
      "top     [{'piece_type': 'other', 'piece_content': 'cla...   \n",
      "freq                                                  231   \n",
      "\n",
      "                                           full_traceback  traceback_type  \\\n",
      "count                                               14118           14118   \n",
      "unique                                               4693             555   \n",
      "top     Traceback (most recent call last):\\nFile \"/hom...  AttributeError   \n",
      "freq                                                  231            2378   \n",
      "\n",
      "                          before_merge_without_docstrings  \\\n",
      "count                                               14118   \n",
      "unique                                              14103   \n",
      "top         def to_dict(self, orient='dict', into=dict...   \n",
      "freq                                                    2   \n",
      "\n",
      "                           after_merge_without_docstrings  \\\n",
      "count                                               14118   \n",
      "unique                                              14039   \n",
      "top         def search_novel(self, query):\\n        qu...   \n",
      "freq                                                    4   \n",
      "\n",
      "       before_merge_docstrings after_merge_docstrings  \\\n",
      "count                    14118                  14118   \n",
      "unique                    2090                   2097   \n",
      "top                         []                     []   \n",
      "freq                     11837                  11822   \n",
      "\n",
      "                             path_to_snippet_before_merge  \\\n",
      "count                                               14118   \n",
      "unique                                              14118   \n",
      "top     buggy_snippets_files/e93817735d3042d739fe86677...   \n",
      "freq                                                    1   \n",
      "\n",
      "                              path_to_snippet_after_merge  \n",
      "count                                               14118  \n",
      "unique                                              14118  \n",
      "top     buggy_snippets_files/e93817735d3042d739fe86677...  \n",
      "freq                                                    1  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14118 entries, 1790 to 625636\n",
      "Data columns (total 16 columns):\n",
      " #   Column                           Non-Null Count  Dtype \n",
      "---  ------                           --------------  ----- \n",
      " 0   after_merge                      14118 non-null  object\n",
      " 1   before_merge                     14118 non-null  object\n",
      " 2   filename                         14118 non-null  object\n",
      " 3   full_file_code_after_merge       14118 non-null  object\n",
      " 4   full_file_code_before_merge      14118 non-null  object\n",
      " 5   function_name                    14118 non-null  object\n",
      " 6   url                              14118 non-null  object\n",
      " 7   source code and errors           14118 non-null  object\n",
      " 8   full_traceback                   14118 non-null  object\n",
      " 9   traceback_type                   14118 non-null  object\n",
      " 10  before_merge_without_docstrings  14118 non-null  object\n",
      " 11  after_merge_without_docstrings   14118 non-null  object\n",
      " 12  before_merge_docstrings          14118 non-null  object\n",
      " 13  after_merge_docstrings           14118 non-null  object\n",
      " 14  path_to_snippet_before_merge     14118 non-null  object\n",
      " 15  path_to_snippet_after_merge      14118 non-null  object\n",
      "dtypes: object(16)\n",
      "memory usage: 1.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open('buggy_dataset/bugfixes_train.pickle', 'rb') as handle:\n",
    "    df = pd.read_pickle(handle)  # Uses Pandas' built-in method\n",
    "\n",
    "print(df.head())\n",
    "print(df.describe())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            before_merge  \\\n",
      "1790   def plot(result_dict_file, show, plot_save_fil...   \n",
      "2433       def stream_logs(self):\\n        \"\"\"Stream ...   \n",
      "26618      def addRecentProjectFile(self, projectFile...   \n",
      "26622      def addSfmAugmentation(self, withMVS=False...   \n",
      "28217      def load_pymathics_doc(self):\\n        if ...   \n",
      "\n",
      "                                             after_merge     traceback_type  \\\n",
      "1790   def plot(result_pickle_file_path, show, plot_s...          TypeError   \n",
      "2433       def stream_logs(self):\\n        \"\"\"Stream ...  FileNotFoundError   \n",
      "26618      def addRecentProjectFile(self, projectFile...            OSError   \n",
      "26622      def addSfmAugmentation(self, withMVS=False...       RuntimeError   \n",
      "28217      def load_pymathics_doc(self):\\n        if ...           KeyError   \n",
      "\n",
      "                                          full_traceback  \n",
      "1790   Traceback (most recent call last):\\nFile \"c:\\p...  \n",
      "2433   / # jupyter-repo2docker https://github.com/yuv...  \n",
      "26618  [2020-05-23 16:12:48,660][ERROR] Traceback (mo...  \n",
      "26622  Traceback (most recent call last):\\nFile \"C:\\U...  \n",
      "28217  $ mathicsserver\\nwarning: database file /home/...  \n"
     ]
    }
   ],
   "source": [
    "df1 = df[[\"before_merge\", \"after_merge\", \"traceback_type\", \"full_traceback\"]]\n",
    "print(df1.head())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import difflib\n",
    "import textwrap\n",
    "\n",
    "# AST functions\n",
    "def get_ast_json(code):\n",
    "    \"\"\"Convert Python code to an AST representation in JSON format.\"\"\"\n",
    "    if not isinstance(code, str) or not code.strip():\n",
    "        print(f\"Skipping empty or invalid code: {repr(code)}\")\n",
    "        return []\n",
    "    try:\n",
    "        # Remove all leading whitespace (including newlines) from the start\n",
    "        code = code.lstrip()\n",
    "        if not code:\n",
    "            print(f\"Code became empty after stripping: {repr(code)}\")\n",
    "            return []\n",
    "        # Dedent to normalize any remaining indentation\n",
    "        code = textwrap.dedent(code)\n",
    "        tree = ast.parse(code)\n",
    "        nodes = [ast.dump(node) for node in ast.walk(tree)]\n",
    "        return nodes\n",
    "    except SyntaxError as e:\n",
    "        print(f\"SyntaxError in code: {repr(code)}\\nError: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error in code: {repr(code)}\\nError: {e}\")\n",
    "        return []\n",
    "\n",
    "def generate_ast_diff_json(old_code, new_code):\n",
    "    \"\"\"Generate a structured JSON diff between old and new ASTs.\"\"\"\n",
    "    old_ast = get_ast_json(old_code)\n",
    "    new_ast = get_ast_json(new_code)\n",
    "\n",
    "    # Compute AST differences\n",
    "    diff = list(difflib.unified_diff(old_ast, new_ast, lineterm=\"\"))\n",
    "\n",
    "    old_ast_json = json.dumps(old_ast, indent=4)\n",
    "    new_ast_json = json.dumps(new_ast, indent=4)\n",
    "    return diff, old_ast_json, new_ast_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:4: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<unknown>:51: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:52: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<unknown>:64: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:65: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<unknown>:8: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<unknown>:11: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<unknown>:11: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<unknown>:14: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\i'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\i'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\i'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\B'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\B'\n",
      "<unknown>:8: SyntaxWarning: invalid escape sequence '\\%'\n",
      "<unknown>:8: SyntaxWarning: invalid escape sequence '\\%'\n",
      "<unknown>:14: SyntaxWarning: invalid escape sequence '\\%'\n",
      "<unknown>:14: SyntaxWarning: invalid escape sequence '\\%'\n",
      "<unknown>:7: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:7: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:25: SyntaxWarning: invalid escape sequence '\\%'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<unknown>:215: SyntaxWarning: invalid escape sequence '\\('\n",
      "<unknown>:215: SyntaxWarning: invalid escape sequence '\\('\n",
      "<unknown>:17: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<unknown>:18: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<unknown>:21: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:21: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<unknown>:107: SyntaxWarning: invalid escape sequence '\\-'\n",
      "<unknown>:107: SyntaxWarning: invalid escape sequence '\\-'\n",
      "<unknown>:69: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<unknown>:68: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<unknown>:47: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<unknown>:51: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<unknown>:52: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<unknown>:47: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<unknown>:63: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<unknown>:63: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<unknown>:8: SyntaxWarning: invalid escape sequence '\\P'\n",
      "<unknown>:9: SyntaxWarning: invalid escape sequence '\\P'\n",
      "<unknown>:8: SyntaxWarning: invalid escape sequence '\\P'\n",
      "<unknown>:9: SyntaxWarning: invalid escape sequence '\\P'\n",
      "<unknown>:6: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<unknown>:10: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<unknown>:57: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<unknown>:6: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<unknown>:10: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<unknown>:57: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<unknown>:6: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<unknown>:69: SyntaxWarning: invalid escape sequence '\\('\n",
      "<unknown>:82: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<unknown>:70: SyntaxWarning: invalid escape sequence '\\('\n",
      "<unknown>:83: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<unknown>:37: SyntaxWarning: invalid escape sequence '\\('\n",
      "<unknown>:42: SyntaxWarning: invalid escape sequence '\\('\n",
      "<unknown>:35: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<unknown>:40: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<unknown>:15: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<unknown>:15: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<unknown>:67: SyntaxWarning: invalid escape sequence '\\('\n",
      "<unknown>:80: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<unknown>:69: SyntaxWarning: invalid escape sequence '\\('\n",
      "<unknown>:82: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<unknown>:46: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:50: SyntaxWarning: invalid escape sequence '\\['\n",
      "<unknown>:57: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<unknown>:62: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<unknown>:97: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:46: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:50: SyntaxWarning: invalid escape sequence '\\['\n",
      "<unknown>:57: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<unknown>:62: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<unknown>:102: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:6: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<unknown>:70: SyntaxWarning: invalid escape sequence '\\('\n",
      "<unknown>:73: SyntaxWarning: invalid escape sequence '\\('\n",
      "<unknown>:9: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<unknown>:10: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:9: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<unknown>:10: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:55: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<unknown>:50: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<unknown>:51: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<unknown>:55: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<unknown>:51: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<unknown>:55: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<unknown>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:48: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:56: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:44: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:52: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:33: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<unknown>:33: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<unknown>:51: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<unknown>:55: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<unknown>:101: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<unknown>:101: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<unknown>:30: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<unknown>:32: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<unknown>:31: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<unknown>:33: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<unknown>:36: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<unknown>:8: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:8: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:59: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<unknown>:116: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<unknown>:464: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<unknown>:59: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<unknown>:116: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<unknown>:472: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<unknown>:13: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<unknown>:12: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<unknown>:84: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<unknown>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:5: SyntaxWarning: invalid escape sequence '\\['\n",
      "<unknown>:14: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:14: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:21: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<unknown>:5: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:5: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:76: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<unknown>:76: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<unknown>:189: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<unknown>:189: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:76: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<unknown>:76: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<unknown>:189: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<unknown>:189: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:21: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<unknown>:10: SyntaxWarning: invalid escape sequence '\\('\n",
      "<unknown>:10: SyntaxWarning: invalid escape sequence '\\('\n",
      "<unknown>:10: SyntaxWarning: invalid escape sequence '\\('\n",
      "<unknown>:16: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<unknown>:23: SyntaxWarning: invalid escape sequence '\\.'\n"
     ]
    }
   ],
   "source": [
    "data = df1.to_dict(orient='records')\n",
    "for i, record in enumerate(data):\n",
    "    try:\n",
    "        old_code = record[\"before_merge\"]\n",
    "        new_code = record[\"after_merge\"]\n",
    "        record[\"ast_diff\"], record[\"old_ast_json\"], record[\"new_ast_json\"] = generate_ast_diff_json(old_code, new_code)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing record {i}: {e}\")\n",
    "        print(f\"Record: {record}\")\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save to a .pkl file\n",
    "with open(\"train_data_postprocessed.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  To read the train data after processing and checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        before_merge  \\\n",
      "0  def plot(result_dict_file, show, plot_save_fil...   \n",
      "1      def stream_logs(self):\\n        \"\"\"Stream ...   \n",
      "2      def addRecentProjectFile(self, projectFile...   \n",
      "3      def addSfmAugmentation(self, withMVS=False...   \n",
      "4      def load_pymathics_doc(self):\\n        if ...   \n",
      "\n",
      "                                         after_merge     traceback_type  \\\n",
      "0  def plot(result_pickle_file_path, show, plot_s...          TypeError   \n",
      "1      def stream_logs(self):\\n        \"\"\"Stream ...  FileNotFoundError   \n",
      "2      def addRecentProjectFile(self, projectFile...            OSError   \n",
      "3      def addSfmAugmentation(self, withMVS=False...       RuntimeError   \n",
      "4      def load_pymathics_doc(self):\\n        if ...           KeyError   \n",
      "\n",
      "                                      full_traceback  \\\n",
      "0  Traceback (most recent call last):\\nFile \"c:\\p...   \n",
      "1  / # jupyter-repo2docker https://github.com/yuv...   \n",
      "2  [2020-05-23 16:12:48,660][ERROR] Traceback (mo...   \n",
      "3  Traceback (most recent call last):\\nFile \"C:\\U...   \n",
      "4  $ mathicsserver\\nwarning: database file /home/...   \n",
      "\n",
      "                                            ast_diff  \\\n",
      "0  [--- , +++ , @@ -1,23 +1,23 @@, -Module(body=[...   \n",
      "1  [--- , +++ , @@ -1,20 +1,26 @@, -Module(body=[...   \n",
      "2  [--- , +++ , @@ -1,7 +1,7 @@, -Module(body=[Fu...   \n",
      "3  [--- , +++ , @@ -1,12 +1,12 @@, -Module(body=[...   \n",
      "4  [--- , +++ , @@ -1,12 +1,12 @@, -Module(body=[...   \n",
      "\n",
      "                                        old_ast_json  \\\n",
      "0  [\\n    \"Module(body=[FunctionDef(name='plot', ...   \n",
      "1  [\\n    \"Module(body=[FunctionDef(name='stream_...   \n",
      "2  [\\n    \"Module(body=[FunctionDef(name='addRece...   \n",
      "3  [\\n    \"Module(body=[FunctionDef(name='addSfmA...   \n",
      "4  [\\n    \"Module(body=[FunctionDef(name='load_py...   \n",
      "\n",
      "                                        new_ast_json  \n",
      "0  [\\n    \"Module(body=[FunctionDef(name='plot', ...  \n",
      "1  [\\n    \"Module(body=[FunctionDef(name='stream_...  \n",
      "2  [\\n    \"Module(body=[FunctionDef(name='addRece...  \n",
      "3  [\\n    \"Module(body=[FunctionDef(name='addSfmA...  \n",
      "4  [\\n    \"Module(body=[FunctionDef(name='load_py...  \n",
      "/n--------------------------------------------------/n\n",
      "                                             before_merge  \\\n",
      "count                                               14118   \n",
      "unique                                              14118   \n",
      "top     def plot(result_dict_file, show, plot_save_fil...   \n",
      "freq                                                    1   \n",
      "\n",
      "                                              after_merge  traceback_type  \\\n",
      "count                                               14118           14118   \n",
      "unique                                              14055             555   \n",
      "top         def search_novel(self, query):\\n        qu...  AttributeError   \n",
      "freq                                                    4            2378   \n",
      "\n",
      "                                           full_traceback ast_diff  \\\n",
      "count                                               14118    14118   \n",
      "unique                                               4693    13878   \n",
      "top     Traceback (most recent call last):\\nFile \"/hom...       []   \n",
      "freq                                                  231      230   \n",
      "\n",
      "                                             old_ast_json  \\\n",
      "count                                               14118   \n",
      "unique                                              14093   \n",
      "top     [\\n    \"Module(body=[AsyncFunctionDef(name='co...   \n",
      "freq                                                    2   \n",
      "\n",
      "                                             new_ast_json  \n",
      "count                                               14118  \n",
      "unique                                              14022  \n",
      "top     [\\n    \"Module(body=[FunctionDef(name='search_...  \n",
      "freq                                                    4  \n",
      "/n--------------------------------------------------/n\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14118 entries, 0 to 14117\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   before_merge    14118 non-null  object\n",
      " 1   after_merge     14118 non-null  object\n",
      " 2   traceback_type  14118 non-null  object\n",
      " 3   full_traceback  14118 non-null  object\n",
      " 4   ast_diff        14118 non-null  object\n",
      " 5   old_ast_json    14118 non-null  object\n",
      " 6   new_ast_json    14118 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 772.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open('train_data_postprocessed.pkl', 'rb') as handle:\n",
    "    df = pd.read_pickle(handle)  # Uses Pandas' built-in method\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "print(df.head())\n",
    "print(\"/n\" + \"-\" * 50 + \"/n\")\n",
    "print(df.describe())\n",
    "print(\"/n\" + \"-\" * 50 + \"/n\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All records processed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Checking each record in the DataFrame so there are no errors\n",
    "\n",
    "for i, record in enumerate(df.to_dict(orient='records')):\n",
    "    try:\n",
    "        assert \"before_merge\" in record\n",
    "        assert \"after_merge\" in record\n",
    "        assert \"traceback_type\" in record\n",
    "        assert \"full_traceback\" in record\n",
    "        assert \"ast_diff\" in record\n",
    "        assert \"old_ast_json\" in record\n",
    "        assert \"new_ast_json\" in record\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing record {i}: {e}\")\n",
    "        print(f\"Record: {record}\")\n",
    "        break\n",
    "print(\"All records processed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No null values in the DataFrame!\n"
     ]
    }
   ],
   "source": [
    "# Checking that there are no null values in the DataFrame\n",
    "assert not df.isnull().values.any()\n",
    "print(\"No null values in the DataFrame!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending this for valid and test pickle files in buggy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyntaxError in code: \"@classmethod\\n    def _load_model_state(cls, checkpoint: Dict[str, Any], *cls_args, **cls_kwargs):\\n        cls_spec = inspect.getfullargspec(cls.__init__)\\n        cls_init_args_name = inspect.signature(cls).parameters.keys()\\n        # pass in the values we saved automatically\\n        if cls.CHECKPOINT_HYPER_PARAMS_KEY in checkpoint:\\n            model_args = {}\\n\\n            # add some back compatibility, the actual one shall be last\\n            for hparam_key in CHECKPOINT_PAST_HPARAMS_KEYS + (cls.CHECKPOINT_HYPER_PARAMS_KEY,):\\n                if hparam_key in checkpoint:\\n                    model_args.update(checkpoint[hparam_key])\\n\\n            model_args = _convert_loaded_hparams(model_args, checkpoint.get(cls.CHECKPOINT_HYPER_PARAMS_TYPE))\\n\\n            args_name = checkpoint.get(cls.CHECKPOINT_HYPER_PARAMS_NAME)\\n\\n            if args_name == 'kwargs':\\n                # in case the class cannot take any extra argument filter only the possible\\n                cls_kwargs.update(**model_args)\\n            elif args_name:\\n                if args_name in cls_init_args_name:\\n                    cls_kwargs.update({args_name: model_args})\\n            else:\\n                cls_args = (model_args,) + cls_args\\n\\n        if not cls_spec.varkw:\\n            # filter kwargs according to class init unless it allows any argument via kwargs\\n            cls_kwargs = {k: v for k, v in cls_kwargs.items() if k in cls_init_args_name}\\n\\n        # prevent passing positional arguments if class does not accept any\\n        if len(cls_spec.args) <= 1 and not cls_spec.kwonlyargs:\\n            cls_args, cls_kwargs = [], {}\\n        model = cls(*cls_args, **cls_kwargs)\\n        # load the state_dict on the model automatically\\n        model.load_state_dict(checkpoint['state_dict'])\\n\\n        # give model a chance to load something\\n        model.on_load_checkpoint(checkpoint)\\n\\n        return model\\n\"\n",
      "Error: unexpected indent (<unknown>, line 2)\n",
      "SyntaxError in code: \"@classmethod\\n    def _load_model_state(cls, checkpoint: Dict[str, Any], *cls_args, **cls_kwargs):\\n        cls_spec = inspect.getfullargspec(cls.__init__)\\n        cls_init_args_name = inspect.signature(cls).parameters.keys()\\n        # pass in the values we saved automatically\\n        if cls.CHECKPOINT_HYPER_PARAMS_KEY in checkpoint:\\n            model_args = {}\\n\\n            # add some back compatibility, the actual one shall be last\\n            for hparam_key in CHECKPOINT_PAST_HPARAMS_KEYS + (cls.CHECKPOINT_HYPER_PARAMS_KEY,):\\n                if hparam_key in checkpoint:\\n                    model_args.update(checkpoint[hparam_key])\\n\\n            model_args = _convert_loaded_hparams(model_args, checkpoint.get(cls.CHECKPOINT_HYPER_PARAMS_TYPE))\\n\\n            args_name = checkpoint.get(cls.CHECKPOINT_HYPER_PARAMS_NAME)\\n\\n            if args_name == 'kwargs':\\n                # in case the class cannot take any extra argument filter only the possible\\n                cls_kwargs.update(**model_args)\\n            elif args_name:\\n                if args_name in cls_init_args_name:\\n                    cls_kwargs.update({args_name: model_args})\\n            else:\\n                cls_args = (model_args,) + cls_args\\n\\n        if not cls_spec.varkw:\\n            # filter kwargs according to class init unless it allows any argument via kwargs\\n            cls_kwargs = {k: v for k, v in cls_kwargs.items() if k in cls_init_args_name}\\n\\n        # prevent passing positional arguments if class does not accept any\\n        if len(cls_spec.args) <= 1 and not cls_spec.varargs and not cls_spec.kwonlyargs:\\n            cls_args, cls_kwargs = [], {}\\n\\n        model = cls(*cls_args, **cls_kwargs)\\n        # load the state_dict on the model automatically\\n        model.load_state_dict(checkpoint['state_dict'])\\n\\n        # give model a chance to load something\\n        model.on_load_checkpoint(checkpoint)\\n\\n        return model\\n\"\n",
      "Error: unexpected indent (<unknown>, line 2)\n",
      "SyntaxError in code: '@contextmanager\\n    def _emit_changed(self):\\n        \"\"\"\\n        A context manager that calls `emit_selection_rows_changed after\\n        changing a selection.\\n        \"\"\"\\n        def map_from_source(rows):\\n            from_src = self.proxy.mapFromSource\\n            index = self.proxy.sourceModel().index\\n            return {from_src(index(row, 0)).row() for row in rows}\\n\\n        old_rows = self._rows.copy()\\n        try:\\n            yield\\n        finally:\\n            deselected = map_from_source(old_rows - self._rows)\\n            selected = map_from_source(self._rows - old_rows)\\n            if selected or deselected:\\n                for model in self._selection_models:\\n                    model.emit_selection_rows_changed(selected, deselected)\\n'\n",
      "Error: unexpected indent (<unknown>, line 2)\n",
      "SyntaxError in code: '@contextmanager\\n    def _emit_changed(self):\\n        \"\"\"\\n        A context manager that calls `emit_selection_rows_changed after\\n        changing a selection.\\n        \"\"\"\\n        def map_from_source(rows):\\n            from_src = self.proxy.mapFromSource\\n            index = self.proxy.sourceModel().index\\n            return {from_src(index(row, 0)).row() for row in rows}\\n\\n        old_rows = self._rows.copy()\\n        try:\\n            yield\\n        finally:\\n            if self.proxy.sourceModel() is not None:\\n                deselected = map_from_source(old_rows - self._rows)\\n                selected = map_from_source(self._rows - old_rows)\\n                if selected or deselected:\\n                    for model in self._selection_models:\\n                        model.emit_selection_rows_changed(selected, deselected)\\n'\n",
      "Error: unexpected indent (<unknown>, line 2)\n",
      "SyntaxError in code: \"@runs_in_hwd_thread\\n    def get_client(self, keystore, force_pair=True, *,\\n                   devices=None, allow_user_interaction=True) -> Optional['KeepKeyClient']:\\n        client = super().get_client(keystore, force_pair,\\n                                    devices=devices,\\n                                    allow_user_interaction=allow_user_interaction)\\n        # returns the client for a given keystore. can use xpub\\n        if client:\\n            client.used()\\n        return client\\n\"\n",
      "Error: unexpected indent (<unknown>, line 2)\n",
      "SyntaxError in code: '@attach_runtime_statistics(u\"{0.__class__.__name__}.{function_name}\")\\n    @call_on_reactor_thread\\n    def data_came_in(self, addr, data):\\n        \"\"\" The callback function that the thread pool will call when there is incoming data.\\n        :param addr: The (IP, port) address tuple of the sender.\\n        :param data: The data received.\\n        \"\"\"\\n        if not self._is_running:\\n            return\\n\\n        ip, port = addr\\n\\n        # decode the packet\\n        try:\\n            packet = decode_packet(data)\\n        except InvalidPacketException as e:\\n            self._logger.error(u\"Invalid packet from [%s:%s], packet=[%s], error=%s\", ip, port, hexlify(data), e)\\n            return\\n\\n        if packet[\\'opcode\\'] == OPCODE_WRQ:\\n            self._logger.error(u\"WRQ is not supported from [%s:%s], packet=[%s]\", ip, port, repr(packet))\\n            return\\n\\n        self._logger.debug(u\"GOT packet opcode[%s] from %s:%s\", packet[\\'opcode\\'], ip, port)\\n        # a new request\\n        if packet[\\'opcode\\'] == OPCODE_RRQ:\\n            self._logger.debug(u\"start handling new request: %s\", packet)\\n            self._handle_new_request(ip, port, packet)\\n            return\\n\\n        if (ip, port, packet[\\'session_id\\']) not in self._session_dict:\\n            self._logger.warn(u\"got non-existing session from %s:%s, id = %s\", ip, port, packet[\\'session_id\\'])\\n            return\\n\\n        # handle the response\\n        session = self._session_dict[(ip, port, packet[\\'session_id\\'])]\\n        self._process_packet(session, packet)\\n\\n        if not session.is_done and not session.is_failed:\\n            return\\n\\n        self._cleanup_session((ip, port, packet[\\'session_id\\']))\\n\\n        # schedule callback\\n        if session.is_failed:\\n            self._logger.info(u\"%s failed\", session)\\n            if session.failure_callback:\\n                callback = lambda cb = session.failure_callback, a = session.address, fn = session.file_name,\\\\\\n                    msg = \"download failed\", ei = session.extra_info: cb(a, fn, msg, ei)\\n                self._callbacks.append(callback)\\n        elif session.is_done:\\n            self._logger.info(u\"%s finished\", session)\\n            if session.success_callback:\\n                callback = lambda cb = session.success_callback, a = session.address, fn = session.file_name,\\\\\\n                    fd = session.file_data, ei = session.extra_info: cb(a, fn, fd, ei)\\n                self._callbacks.append(callback)\\n\\n        self._schedule_callback_processing()\\n'\n",
      "Error: unexpected indent (<unknown>, line 2)\n",
      "SyntaxError in code: '@attach_runtime_statistics(u\"{0.__class__.__name__}.{function_name}\")\\n    @call_on_reactor_thread\\n    def data_came_in(self, addr, data):\\n        \"\"\" The callback function that the thread pool will call when there is incoming data.\\n        :param addr: The (IP, port) address tuple of the sender.\\n        :param data: The data received.\\n        \"\"\"\\n        if not self._is_running or not is_valid_address(addr):\\n            return\\n\\n        ip, port = addr\\n\\n        # decode the packet\\n        try:\\n            packet = decode_packet(data)\\n        except InvalidPacketException as e:\\n            self._logger.error(u\"Invalid packet from [%s:%s], packet=[%s], error=%s\", ip, port, hexlify(data), e)\\n            return\\n\\n        if packet[\\'opcode\\'] == OPCODE_WRQ:\\n            self._logger.error(u\"WRQ is not supported from [%s:%s], packet=[%s]\", ip, port, repr(packet))\\n            return\\n\\n        self._logger.debug(u\"GOT packet opcode[%s] from %s:%s\", packet[\\'opcode\\'], ip, port)\\n        # a new request\\n        if packet[\\'opcode\\'] == OPCODE_RRQ:\\n            self._logger.debug(u\"start handling new request: %s\", packet)\\n            self._handle_new_request(ip, port, packet)\\n            return\\n\\n        if (ip, port, packet[\\'session_id\\']) not in self._session_dict:\\n            self._logger.warn(u\"got non-existing session from %s:%s, id = %s\", ip, port, packet[\\'session_id\\'])\\n            return\\n\\n        # handle the response\\n        session = self._session_dict[(ip, port, packet[\\'session_id\\'])]\\n        self._process_packet(session, packet)\\n\\n        if not session.is_done and not session.is_failed:\\n            return\\n\\n        self._cleanup_session((ip, port, packet[\\'session_id\\']))\\n\\n        # schedule callback\\n        if session.is_failed:\\n            self._logger.info(u\"%s failed\", session)\\n            if session.failure_callback:\\n                callback = lambda cb = session.failure_callback, a = session.address, fn = session.file_name,\\\\\\n                    msg = \"download failed\", ei = session.extra_info: cb(a, fn, msg, ei)\\n                self._callbacks.append(callback)\\n        elif session.is_done:\\n            self._logger.info(u\"%s finished\", session)\\n            if session.success_callback:\\n                callback = lambda cb = session.success_callback, a = session.address, fn = session.file_name,\\\\\\n                    fd = session.file_data, ei = session.extra_info: cb(a, fn, fd, ei)\\n                self._callbacks.append(callback)\\n\\n        self._schedule_callback_processing()\\n'\n",
      "Error: unexpected indent (<unknown>, line 2)\n",
      "SyntaxError in code: '@classmethod\\n    def write_content(cls, filename, content, rewrite_html=True):\\n        \"\"\"Write content to file.\"\"\"\\n        if rewrite_html:\\n            doc = html.document_fromstring(content)\\n            doc.rewrite_links(replacer)\\n            content = html.tostring(doc, encoding=\\'utf8\\')\\n        else:\\n            content = content.encode(\\'utf-8\\')\\n\\n        utils.makedirs(os.path.dirname(filename))\\n        with open(filename, \"wb+\") as fd:\\n            fd.write(content)\\n'\n",
      "Error: unexpected indent (<unknown>, line 2)\n",
      "SyntaxError in code: '@classmethod\\n    def write_content(cls, filename, content, rewrite_html=True):\\n        \"\"\"Write content to file.\"\"\"\\n        if rewrite_html:\\n            try:\\n                doc = html.document_fromstring(content)\\n                doc.rewrite_links(replacer)\\n                content = html.tostring(doc, encoding=\\'utf8\\')\\n            except etree.ParserError:\\n                content = content.encode(\\'utf-8\\')\\n        else:\\n            content = content.encode(\\'utf-8\\')\\n\\n        utils.makedirs(os.path.dirname(filename))\\n        with open(filename, \"wb+\") as fd:\\n            fd.write(content)\\n'\n",
      "Error: unexpected indent (<unknown>, line 2)\n",
      "SyntaxError in code: '@classmethod\\n    def from_line(cls, line):\\n        \"\"\"Create a Log Line from a string line.\\n\\n        :param line:\\n        :type line: str\\n        :return:\\n        :rtype: LogLine or None\\n        \"\"\"\\n        lines = line.split(\\'\\\\n\\')\\n        match = LogLine.log_re.match(lines[0])\\n        if not match:\\n            return\\n\\n        g = match.groupdict()\\n        return LogLine(line=lines[0], message=g[\\'message\\'], level_name=g[\\'level_name\\'], extra=g.get(\\'extra\\'), curhash=g[\\'curhash\\'],\\n                       thread_name=g[\\'thread_name\\'], thread_id=int(g[\\'thread_id\\']) if g[\\'thread_id\\'] else None, traceback_lines=lines[1:],\\n                       timestamp=datetime.datetime(year=int(g[\\'year\\']), month=int(g[\\'month\\']), day=int(g[\\'day\\']),\\n                                                   hour=int(g[\\'hour\\']), minute=int(g[\\'minute\\']), second=int(g[\\'second\\'])))\\n'\n",
      "Error: unexpected indent (<unknown>, line 2)\n",
      "SyntaxError in code: '@classmethod\\n    def from_line(cls, line):\\n        \"\"\"Create a Log Line from a string line.\\n\\n        :param line:\\n        :type line: str\\n        :return:\\n        :rtype: LogLine or None\\n        \"\"\"\\n        lines = line.split(b\\'\\\\n\\')\\n        match = LogLine.log_re.match(lines[0])\\n        if not match:\\n            return\\n\\n        g = match.groupdict()\\n        return LogLine(line=lines[0], message=g[\\'message\\'], level_name=g[\\'level_name\\'], extra=g.get(\\'extra\\'), curhash=g[\\'curhash\\'],\\n                       thread_name=g[\\'thread_name\\'], thread_id=int(g[\\'thread_id\\']) if g[\\'thread_id\\'] else None, traceback_lines=lines[1:],\\n                       timestamp=datetime.datetime(year=int(g[\\'year\\']), month=int(g[\\'month\\']), day=int(g[\\'day\\']),\\n                                                   hour=int(g[\\'hour\\']), minute=int(g[\\'minute\\']), second=int(g[\\'second\\'])))\\n'\n",
      "Error: unexpected indent (<unknown>, line 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<unknown>:52: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:53: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:55: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:52: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:53: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:55: SyntaxWarning: invalid escape sequence '\\e'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyntaxError in code: '@command.command(\"export.file\")\\n    def file(self, fmt: str, f: flow.Flow, path: mitmproxy.types.Path) -> None:\\n        \"\"\"\\n            Export a flow to path.\\n        \"\"\"\\n        if fmt not in formats:\\n            raise exceptions.CommandError(\"No such export format: %s\" % fmt)\\n        func = formats[fmt]  # type: typing.Any\\n        v = func(f)\\n        with open(path, \"wb\") as fp:\\n            if isinstance(v, bytes):\\n                fp.write(v)\\n            else:\\n                fp.write(v.encode(\"utf-8\"))\\n'\n",
      "Error: unexpected indent (<unknown>, line 2)\n",
      "SyntaxError in code: '@command.command(\"export.file\")\\n    def file(self, fmt: str, f: flow.Flow, path: mitmproxy.types.Path) -> None:\\n        \"\"\"\\n            Export a flow to path.\\n        \"\"\"\\n        if fmt not in formats:\\n            raise exceptions.CommandError(\"No such export format: %s\" % fmt)\\n        func = formats[fmt]  # type: typing.Any\\n        v = func(f)\\n        try:\\n            with open(path, \"wb\") as fp:\\n                if isinstance(v, bytes):\\n                    fp.write(v)\\n                else:\\n                    fp.write(v.encode(\"utf-8\"))\\n        except IOError as e:\\n            ctx.log.error(str(e))\\n'\n",
      "Error: unexpected indent (<unknown>, line 2)\n",
      "SyntaxError in code: '@audioset.command()\\n    @checks.mod_or_permissions(administrator=True)\\n    async def thumbnail(self, ctx):\\n        \"\"\"Toggle displaying a thumbnail on audio messages.\"\"\"\\n        thumbnail = await self.config.guild(ctx.guild).thumbnail()\\n        await self.config.guild(ctx.guild).thumbnail.set(not thumbnail)\\n        await self._embed_msg(ctx, _(\"Thumbnail display: {}.\").format(not thumbnail))\\n'\n",
      "Error: unexpected indent (<unknown>, line 2)\n",
      "SyntaxError in code: '@audioset.command()\\n    @checks.mod_or_permissions(administrator=True)\\n    async def thumbnail(self, ctx):\\n        \"\"\"Toggle displaying a thumbnail on audio messages.\"\"\"\\n        thumbnail = await self.config.guild(ctx.guild).thumbnail()\\n        await self.config.guild(ctx.guild).thumbnail.set(not thumbnail)\\n        await self._embed_msg(\\n            ctx, _(\"Thumbnail display: {true_or_false}.\").format(true_or_false=not thumbnail)\\n        )'\n",
      "Error: unexpected indent (<unknown>, line 2)\n",
      "SyntaxError in code: '@blocking_call_on_reactor_thread\\n    def load_communities(self):\\n        self._logger.info(\"tribler: Preparing communities...\")\\n        now_time = timemod.time()\\n        default_kwargs = {\\'tribler_session\\': self.session}\\n\\n        # Search Community\\n        if self.session.get_enable_torrent_search():\\n            from Tribler.community.search.community import SearchCommunity\\n            self.dispersy.define_auto_load(SearchCommunity, self.session.dispersy_member, load=True,\\n                                           kargs=default_kwargs)\\n\\n        # AllChannel Community\\n        if self.session.get_enable_channel_search():\\n            from Tribler.community.allchannel.community import AllChannelCommunity\\n            self.dispersy.define_auto_load(AllChannelCommunity, self.session.dispersy_member, load=True,\\n                                           kargs=default_kwargs)\\n\\n        # Channel Community\\n        if self.session.get_channel_community_enabled():\\n            from Tribler.community.channel.community import ChannelCommunity\\n            self.dispersy.define_auto_load(ChannelCommunity,\\n                                           self.session.dispersy_member, load=True, kargs=default_kwargs)\\n\\n        # PreviewChannel Community\\n        if self.session.get_preview_channel_community_enabled():\\n            from Tribler.community.channel.preview import PreviewChannelCommunity\\n            self.dispersy.define_auto_load(PreviewChannelCommunity,\\n                                           self.session.dispersy_member, kargs=default_kwargs)\\n\\n        if self.session.get_tunnel_community_enabled():\\n            tunnel_settings = TunnelSettings(tribler_session=self.session)\\n            tunnel_kwargs = {\\'tribler_session\\': self.session, \\'settings\\': tunnel_settings}\\n\\n            if self.session.get_enable_multichain():\\n                multichain_kwargs = {\\'tribler_session\\': self.session}\\n\\n                # If the multichain is enabled, we use the permanent multichain keypair\\n                # for both the multichain and the tunnel community\\n                keypair = self.session.multichain_keypair\\n                dispersy_member = self.dispersy.get_member(private_key=keypair.key_to_bin())\\n\\n                from Tribler.community.multichain.community import MultiChainCommunity\\n                self.dispersy.define_auto_load(MultiChainCommunity,\\n                                               dispersy_member,\\n                                               load=True,\\n                                               kargs=multichain_kwargs)\\n\\n            else:\\n                keypair = self.dispersy.crypto.generate_key(u\"curve25519\")\\n                dispersy_member = self.dispersy.get_member(private_key=self.dispersy.crypto.key_to_bin(keypair))\\n\\n            from Tribler.community.tunnel.hidden_community import HiddenTunnelCommunity\\n            self.tunnel_community = self.dispersy.define_auto_load(HiddenTunnelCommunity, dispersy_member,\\n                                                                   load=True, kargs=tunnel_kwargs)[0]\\n\\n        self._logger.info(\"tribler: communities are ready in %.2f seconds\", timemod.time() - now_time)\\n'\n",
      "Error: unexpected indent (<unknown>, line 2)\n",
      "SyntaxError in code: '@blocking_call_on_reactor_thread\\n    def load_communities(self):\\n        self._logger.info(\"tribler: Preparing communities...\")\\n        now_time = timemod.time()\\n        default_kwargs = {\\'tribler_session\\': self.session}\\n\\n        # Search Community\\n        if self.session.get_enable_torrent_search():\\n            from Tribler.community.search.community import SearchCommunity\\n            self.dispersy.define_auto_load(SearchCommunity, self.session.dispersy_member, load=True,\\n                                           kargs=default_kwargs)\\n\\n        # AllChannel Community\\n        if self.session.get_enable_channel_search():\\n            from Tribler.community.allchannel.community import AllChannelCommunity\\n            self.dispersy.define_auto_load(AllChannelCommunity, self.session.dispersy_member, load=True,\\n                                           kargs=default_kwargs)\\n\\n        # Channel Community\\n        if self.session.get_channel_community_enabled():\\n            from Tribler.community.channel.community import ChannelCommunity\\n            self.dispersy.define_auto_load(ChannelCommunity,\\n                                           self.session.dispersy_member, load=True, kargs=default_kwargs)\\n\\n        # PreviewChannel Community\\n        if self.session.get_preview_channel_community_enabled():\\n            from Tribler.community.channel.preview import PreviewChannelCommunity\\n            self.dispersy.define_auto_load(PreviewChannelCommunity,\\n                                           self.session.dispersy_member, kargs=default_kwargs)\\n\\n        if self.session.get_tunnel_community_enabled():\\n            tunnel_settings = TunnelSettings(tribler_session=self.session)\\n            tunnel_kwargs = {\\'tribler_session\\': self.session, \\'settings\\': tunnel_settings}\\n\\n            if self.session.get_enable_multichain():\\n                multichain_kwargs = {\\'tribler_session\\': self.session}\\n\\n                # If the multichain is enabled, we use the permanent multichain keypair\\n                # for both the multichain and the tunnel community\\n                keypair = self.session.multichain_keypair\\n                dispersy_member = self.dispersy.get_member(private_key=keypair.key_to_bin())\\n\\n                from Tribler.community.multichain.community import MultiChainCommunity\\n                self.dispersy.define_auto_load(MultiChainCommunity,\\n                                               dispersy_member,\\n                                               load=True,\\n                                               kargs=multichain_kwargs)\\n\\n            else:\\n                keypair = self.dispersy.crypto.generate_key(u\"curve25519\")\\n                dispersy_member = self.dispersy.get_member(private_key=self.dispersy.crypto.key_to_bin(keypair))\\n\\n            from Tribler.community.tunnel.hidden_community import HiddenTunnelCommunity\\n            self.tunnel_community = self.dispersy.define_auto_load(HiddenTunnelCommunity, dispersy_member,\\n                                                                   load=True, kargs=tunnel_kwargs)[0]\\n\\n            # We don\\'t want to automatically load other instances of this community with other master members.\\n            self.dispersy.undefine_auto_load(HiddenTunnelCommunity)\\n\\n        self._logger.info(\"tribler: communities are ready in %.2f seconds\", timemod.time() - now_time)\\n'\n",
      "Error: unexpected indent (<unknown>, line 2)\n",
      "SyntaxError in code: \"@classmethod\\n    def from_incomplete(cls, *, state, data):\\n        guild_id = int(data['guild']['id'])\\n        channel_id = int(data['channel']['id'])\\n        guild = state._get_guild(guild_id)\\n        if guild is not None:\\n            channel = guild.get_channel(channel_id)\\n        else:\\n            channel_data = data['channel']\\n            guild_data = data['guild']\\n            channel_type = try_enum(ChannelType, channel_data['type'])\\n            channel = PartialInviteChannel(id=channel_id, name=channel_data['name'], type=channel_type)\\n            guild = PartialInviteGuild(state, guild_data, guild_id)\\n        data['guild'] = guild\\n        data['channel'] = channel\\n        return cls(state=state, data=data)\\n\"\n",
      "Error: unexpected indent (<unknown>, line 2)\n",
      "SyntaxError in code: \"@classmethod\\n    def from_incomplete(cls, *, state, data):\\n        try:\\n            guild_id = int(data['guild']['id'])\\n        except KeyError:\\n            # If we're here, then this is a group DM\\n            guild = None\\n        else:\\n            guild = state._get_guild(guild_id)\\n            if guild is None:\\n                # If it's not cached, then it has to be a partial guild\\n                guild_data = data['guild']\\n                guild = PartialInviteGuild(state, guild_data, guild_id)\\n\\n        # As far as I know, invites always need a channel\\n        # So this should never raise.\\n        channel_data = data['channel']\\n        channel_id = int(channel_data['id'])\\n        channel_type = try_enum(ChannelType, channel_data['type'])\\n        channel = PartialInviteChannel(id=channel_id, name=channel_data['name'], type=channel_type)\\n        if guild is not None:\\n            # Upgrade the partial data if applicable\\n            channel = guild.get_channel(channel_id) or channel\\n\\n        data['guild'] = guild\\n        data['channel'] = channel\\n        return cls(state=state, data=data)\\n\"\n",
      "Error: unexpected indent (<unknown>, line 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<unknown>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:3: SyntaxWarning: invalid escape sequence '\\s'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyntaxError in code: '@classmethod\\n    def get_deps_from_req(cls, req, resolver=None):\\n        # type: (Requirement, Optional[\"Resolver\"]) -> Tuple[Set[str], Dict[str, Dict[str, Union[str, bool, List[str]]]]]\\n        from .vendor.requirementslib.models.utils import _requirement_to_str_lowercase_name\\n        from .vendor.requirementslib.models.requirements import Requirement\\n        from requirementslib.utils import is_installable_dir\\n        # TODO: this is way too complex, refactor this\\n        constraints = set()  # type: Set[str]\\n        locked_deps = dict()  # type: Dict[str, Dict[str, Union[str, bool, List[str]]]]\\n        if (req.is_file_or_url or req.is_vcs) and not req.is_wheel:\\n            # for local packages with setup.py files and potential direct url deps:\\n            if req.is_vcs:\\n                req_list, lockfile = get_vcs_deps(reqs=[req])\\n                req = next(iter(req for req in req_list if req is not None), req_list)\\n                entry = lockfile[pep423_name(req.normalized_name)]\\n            else:\\n                _, entry = req.pipfile_entry\\n            parsed_line = req.req.parsed_line  # type: Line\\n            setup_info = None  # type: Any\\n            try:\\n                name = req.normalized_name\\n            except TypeError:\\n                raise RequirementError(req=req)\\n            setup_info = req.req.setup_info\\n            setup_info.get_info()\\n            locked_deps[pep423_name(name)] = entry\\n            requirements = []\\n            # Allow users to toggle resolution off for non-editable VCS packages\\n            # but leave it on for local, installable folders on the filesystem\\n            if environments.PIPENV_RESOLVE_VCS or (\\n                req.editable or parsed_line.is_wheel or (\\n                    req.is_file_or_url and parsed_line.is_local\\n                    and is_installable_dir(parsed_line.path)\\n                )\\n            ):\\n                requirements = [v for v in getattr(setup_info, \"requires\", {}).values()]\\n            for r in requirements:\\n                if getattr(r, \"url\", None) and not getattr(r, \"editable\", False):\\n                    if r is not None:\\n                        if not r.url:\\n                            continue\\n                        line = _requirement_to_str_lowercase_name(r)\\n                        new_req, _, _ = cls.parse_line(line)\\n                        if r.marker and not r.marker.evaluate():\\n                            new_constraints = {}\\n                            _, new_entry = req.pipfile_entry\\n                            new_lock = {\\n                                pep423_name(new_req.normalized_name): new_entry\\n                            }\\n                        else:\\n                            new_constraints, new_lock = cls.get_deps_from_req(\\n                                new_req, resolver\\n                            )\\n                        locked_deps.update(new_lock)\\n                        constraints |= new_constraints\\n                # if there is no marker or there is a valid marker, add the constraint line\\n                elif r and (not r.marker or (r.marker and r.marker.evaluate())):\\n                    line = _requirement_to_str_lowercase_name(r)\\n                    constraints.add(line)\\n            # ensure the top level entry remains as provided\\n            # note that we shouldn\\'t pin versions for editable vcs deps\\n            if not req.is_vcs:\\n                if req.specifiers:\\n                    locked_deps[name][\"version\"] = req.specifiers\\n                elif parsed_line.setup_info and parsed_line.setup_info.version:\\n                    locked_deps[name][\"version\"] = \"=={}\".format(\\n                        parsed_line.setup_info.version\\n                    )\\n            # if not req.is_vcs:\\n            locked_deps.update({name: entry})\\n            if req.is_vcs and req.editable:\\n                constraints.add(req.constraint_line)\\n            if req.is_file_or_url and req.req.is_local and req.editable and (\\n                    req.req.setup_path is not None and os.path.exists(req.req.setup_path)):\\n                constraints.add(req.constraint_line)\\n        else:\\n            # if the dependency isn\\'t installable, don\\'t add it to constraints\\n            # and instead add it directly to the lock\\n            if req and req.requirement and (\\n                req.requirement.marker and not req.requirement.marker.evaluate()\\n            ):\\n                pypi = resolver.repository if resolver else None\\n                best_match = pypi.find_best_match(req.ireq) if pypi else None\\n                if best_match:\\n                    hashes = resolver.collect_hashes(best_match) if resolver else []\\n                    new_req = Requirement.from_ireq(best_match)\\n                    new_req = new_req.add_hashes(hashes)\\n                    name, entry = new_req.pipfile_entry\\n                    locked_deps[pep423_name(name)] = translate_markers(entry)\\n                return constraints, locked_deps\\n            constraints.add(req.constraint_line)\\n            return constraints, locked_deps\\n        return constraints, locked_deps\\n'\n",
      "Error: unexpected indent (<unknown>, line 2)\n",
      "SyntaxError in code: '@classmethod\\n    def get_deps_from_req(cls, req, resolver=None):\\n        # type: (Requirement, Optional[\"Resolver\"]) -> Tuple[Set[str], Dict[str, Dict[str, Union[str, bool, List[str]]]]]\\n        from .patched.piptools.exceptions import NoCandidateFound\\n        from .vendor.requirementslib.models.utils import _requirement_to_str_lowercase_name\\n        from .vendor.requirementslib.models.requirements import Requirement\\n        from .vendor.requirementslib.utils import is_installable_dir\\n        # TODO: this is way too complex, refactor this\\n        constraints = set()  # type: Set[str]\\n        locked_deps = dict()  # type: Dict[str, Dict[str, Union[str, bool, List[str]]]]\\n        if (req.is_file_or_url or req.is_vcs) and not req.is_wheel:\\n            # for local packages with setup.py files and potential direct url deps:\\n            if req.is_vcs:\\n                req_list, lockfile = get_vcs_deps(reqs=[req])\\n                req = next(iter(req for req in req_list if req is not None), req_list)\\n                entry = lockfile[pep423_name(req.normalized_name)]\\n            else:\\n                _, entry = req.pipfile_entry\\n            parsed_line = req.req.parsed_line  # type: Line\\n            setup_info = None  # type: Any\\n            try:\\n                name = req.normalized_name\\n            except TypeError:\\n                raise RequirementError(req=req)\\n            setup_info = req.req.setup_info\\n            setup_info.get_info()\\n            locked_deps[pep423_name(name)] = entry\\n            requirements = []\\n            # Allow users to toggle resolution off for non-editable VCS packages\\n            # but leave it on for local, installable folders on the filesystem\\n            if environments.PIPENV_RESOLVE_VCS or (\\n                req.editable or parsed_line.is_wheel or (\\n                    req.is_file_or_url and parsed_line.is_local\\n                    and is_installable_dir(parsed_line.path)\\n                )\\n            ):\\n                requirements = [v for v in getattr(setup_info, \"requires\", {}).values()]\\n            for r in requirements:\\n                if getattr(r, \"url\", None) and not getattr(r, \"editable\", False):\\n                    if r is not None:\\n                        if not r.url:\\n                            continue\\n                        line = _requirement_to_str_lowercase_name(r)\\n                        new_req, _, _ = cls.parse_line(line)\\n                        if r.marker and not r.marker.evaluate():\\n                            new_constraints = {}\\n                            _, new_entry = req.pipfile_entry\\n                            new_lock = {\\n                                pep423_name(new_req.normalized_name): new_entry\\n                            }\\n                        else:\\n                            new_constraints, new_lock = cls.get_deps_from_req(\\n                                new_req, resolver\\n                            )\\n                        locked_deps.update(new_lock)\\n                        constraints |= new_constraints\\n                # if there is no marker or there is a valid marker, add the constraint line\\n                elif r and (not r.marker or (r.marker and r.marker.evaluate())):\\n                    line = _requirement_to_str_lowercase_name(r)\\n                    constraints.add(line)\\n            # ensure the top level entry remains as provided\\n            # note that we shouldn\\'t pin versions for editable vcs deps\\n            if not req.is_vcs:\\n                if req.specifiers:\\n                    locked_deps[name][\"version\"] = req.specifiers\\n                elif parsed_line.setup_info and parsed_line.setup_info.version:\\n                    locked_deps[name][\"version\"] = \"=={}\".format(\\n                        parsed_line.setup_info.version\\n                    )\\n            # if not req.is_vcs:\\n            locked_deps.update({name: entry})\\n            if req.is_vcs and req.editable:\\n                constraints.add(req.constraint_line)\\n            if req.is_file_or_url and req.req.is_local and req.editable and (\\n                    req.req.setup_path is not None and os.path.exists(req.req.setup_path)):\\n                constraints.add(req.constraint_line)\\n        else:\\n            # if the dependency isn\\'t installable, don\\'t add it to constraints\\n            # and instead add it directly to the lock\\n            if req and req.requirement and (\\n                req.requirement.marker and not req.requirement.marker.evaluate()\\n            ):\\n                pypi = resolver.repository if resolver else None\\n                try:\\n                    best_match = pypi.find_best_match(req.ireq) if pypi else None\\n                except NoCandidateFound:\\n                    best_match = None\\n                if best_match:\\n                    hashes = resolver.collect_hashes(best_match) if resolver else []\\n                    new_req = Requirement.from_ireq(best_match)\\n                    new_req = new_req.add_hashes(hashes)\\n                    name, entry = new_req.pipfile_entry\\n                    locked_deps[pep423_name(name)] = translate_markers(entry)\\n                    click_echo(\\n                        \"{} doesn\\'t match your environment, \"\\n                        \"its dependencies won\\'t be resolved.\".format(req.as_line()),\\n                        err=True\\n                    )\\n                else:\\n                    click_echo(\\n                        \"Could not find a version of {} that matches your environment, \"\\n                        \"it will be skipped.\".format(req.as_line()),\\n                        err=True\\n                    )\\n                return constraints, locked_deps\\n            constraints.add(req.constraint_line)\\n            return constraints, locked_deps\\n        return constraints, locked_deps\\n'\n",
      "Error: unexpected indent (<unknown>, line 2)\n",
      "SyntaxError in code: \"@staticmethod\\n    def sanitize(value):\\n        value = value.strip('.')\\n        if value:\\n            return value.encode('idna').decode().lower()\\n\"\n",
      "Error: unexpected indent (<unknown>, line 2)\n",
      "SyntaxError in code: \"@staticmethod\\n    def sanitize(value):\\n        value = value.strip('.')\\n        if value:\\n            try:\\n                return value.encode('idna').decode().lower()\\n            except UnicodeError:\\n                return\\n\"\n",
      "Error: unexpected indent (<unknown>, line 2)\n",
      "SyntaxError in code: \"@staticmethod\\n    def sanitize(value):\\n        value = value.rstrip('.')\\n        if value:\\n            return value.encode('idna').decode().lower()\\n\"\n",
      "Error: unexpected indent (<unknown>, line 2)\n",
      "SyntaxError in code: \"@staticmethod\\n    def sanitize(value):\\n        value = value.strip('.')\\n        if value:\\n            return value.encode('idna').decode().lower()\\n\"\n",
      "Error: unexpected indent (<unknown>, line 2)\n"
     ]
    }
   ],
   "source": [
    "with open('buggy_dataset/bugfixes_test.pickle', 'rb') as handle:\n",
    "    df99 = pd.read_pickle(handle)  # Uses Pandas' built-in method\n",
    "\n",
    "df100 = df99[[\"before_merge\", \"after_merge\", \"traceback_type\", \"full_traceback\"]]\n",
    "\n",
    "data = df100.to_dict(orient='records')\n",
    "for i, record in enumerate(data):\n",
    "    try:\n",
    "        old_code = record[\"before_merge\"]\n",
    "        new_code = record[\"after_merge\"]\n",
    "        record[\"ast_diff\"], record[\"old_ast_json\"], record[\"new_ast_json\"] = generate_ast_diff_json(old_code, new_code)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing record {i}: {e}\")\n",
    "        print(f\"Record: {record}\")\n",
    "        break\n",
    "\n",
    "# Save to a .pkl file\n",
    "with open(\"test_data_postprocessed.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<unknown>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:15: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:52: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:53: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:55: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:52: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:53: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:55: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\<'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\<'\n",
      "<unknown>:32: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:33: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:35: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:32: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:33: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:35: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:31: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:32: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:34: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:32: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:33: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:35: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:31: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:32: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:34: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:31: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:32: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:34: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:62: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:63: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:62: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:63: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:176: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<unknown>:175: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<unknown>:30: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:30: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:16: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<unknown>:10: SyntaxWarning: invalid escape sequence '\\-'\n",
      "<unknown>:42: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<unknown>:43: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<unknown>:20: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:21: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:14: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<unknown>:14: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<unknown>:14: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<unknown>:14: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<unknown>:3: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<unknown>:3: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<unknown>:10: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:11: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:12: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:14: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:15: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:16: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:8: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:9: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:10: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:10: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:11: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:12: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:13: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:13: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:26: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:26: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:7: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:8: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:7: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:8: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:60: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<unknown>:60: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<unknown>:14: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<unknown>:14: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<unknown>:14: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<unknown>:17: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:24: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:17: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:24: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:94: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<unknown>:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:96: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<unknown>:97: SyntaxWarning: invalid escape sequence '\\('\n",
      "<unknown>:12: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:79: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:176: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:291: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:321: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:401: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:419: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:442: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:514: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:540: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:561: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:587: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:621: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:656: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:691: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:707: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:774: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:72: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:157: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:272: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:302: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:382: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:400: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:423: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:495: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:521: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:542: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:568: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:601: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:635: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:669: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:685: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:752: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:69: SyntaxWarning: invalid escape sequence '\\('\n",
      "<unknown>:69: SyntaxWarning: invalid escape sequence '\\('\n",
      "<unknown>:7: SyntaxWarning: invalid escape sequence '\\+'\n",
      "<unknown>:7: SyntaxWarning: invalid escape sequence '\\+'\n",
      "<unknown>:87: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<unknown>:87: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<unknown>:3: SyntaxWarning: invalid escape sequence '\\['\n",
      "<unknown>:3: SyntaxWarning: invalid escape sequence '\\['\n",
      "<unknown>:47: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<unknown>:47: SyntaxWarning: invalid escape sequence '\\/'\n"
     ]
    }
   ],
   "source": [
    "with open('buggy_dataset/bugfixes_valid.pickle', 'rb') as handle:\n",
    "    df990 = pd.read_pickle(handle)  # Uses Pandas' built-in method\n",
    "\n",
    "df1000 = df990[[\"before_merge\", \"after_merge\", \"traceback_type\", \"full_traceback\"]]\n",
    "\n",
    "data = df1000.to_dict(orient='records')\n",
    "for i, record in enumerate(data):\n",
    "    try:\n",
    "        old_code = record[\"before_merge\"]\n",
    "        new_code = record[\"after_merge\"]\n",
    "        record[\"ast_diff\"], record[\"old_ast_json\"], record[\"new_ast_json\"] = generate_ast_diff_json(old_code, new_code)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing record {i}: {e}\")\n",
    "        print(f\"Record: {record}\")\n",
    "        break\n",
    "\n",
    "# Save to a .pkl file\n",
    "with open(\"valid_data_postprocessed.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checks on test pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        before_merge  \\\n",
      "0  def remove_lb_backend_address_pool_address(cmd...   \n",
      "1      def split_action(arguments):\\n        clas...   \n",
      "2      def parse_series(self, data, **kwargs):\\n ...   \n",
      "3      def __init__(self, **kwargs):\\n        # S...   \n",
      "4      def dump_checkpoint(self, weights_only: bo...   \n",
      "\n",
      "                                         after_merge  \\\n",
      "0  def remove_lb_backend_address_pool_address(cmd...   \n",
      "1      def split_action(arguments):\\n        clas...   \n",
      "2      def parse_series(self, data, **kwargs):\\n ...   \n",
      "3      def __init__(self, **kwargs):\\n        # S...   \n",
      "4      def dump_checkpoint(self, weights_only: bo...   \n",
      "\n",
      "                  traceback_type  \\\n",
      "0                 AttributeError   \n",
      "1                 AttributeError   \n",
      "2                 AttributeError   \n",
      "3  pygmt.exceptions.GMTCLibError   \n",
      "4                 AttributeError   \n",
      "\n",
      "                                      full_traceback  \\\n",
      "0  john@Azure:~$ az network lb address-pool addre...   \n",
      "1  'str' object has no attribute 'append'\\nTraceb...   \n",
      "2  2018-12-10 19:39 DEBUG    parser_guessit movin...   \n",
      "3  pygmt-session [ERROR]: Syntax error: Unrecogni...   \n",
      "4  Running command:\\npython pipe/train_cnn.py\\n/h...   \n",
      "\n",
      "                                            ast_diff  \\\n",
      "0  [--- , +++ , @@ -1,8 +1,9 @@, -Module(body=[Fu...   \n",
      "1  [--- , +++ , @@ -1,17 +1,17 @@, -Module(body=[...   \n",
      "2  [--- , +++ , @@ -1,5 +1,5 @@, -Module(body=[Fu...   \n",
      "3  [--- , +++ , @@ -1,27 +1,48 @@, -Module(body=[...   \n",
      "4  [--- , +++ , @@ -1,9 +1,9 @@, -Module(body=[Fu...   \n",
      "\n",
      "                                        old_ast_json  \\\n",
      "0  [\\n    \"Module(body=[FunctionDef(name='remove_...   \n",
      "1  [\\n    \"Module(body=[FunctionDef(name='split_a...   \n",
      "2  [\\n    \"Module(body=[FunctionDef(name='parse_s...   \n",
      "3  [\\n    \"Module(body=[FunctionDef(name='__init_...   \n",
      "4  [\\n    \"Module(body=[FunctionDef(name='dump_ch...   \n",
      "\n",
      "                                        new_ast_json  \n",
      "0  [\\n    \"Module(body=[FunctionDef(name='remove_...  \n",
      "1  [\\n    \"Module(body=[FunctionDef(name='split_a...  \n",
      "2  [\\n    \"Module(body=[FunctionDef(name='parse_s...  \n",
      "3  [\\n    \"Module(body=[FunctionDef(name='__init_...  \n",
      "4  [\\n    \"Module(body=[FunctionDef(name='dump_ch...  \n",
      "/n--------------------------------------------------/n\n",
      "                                             before_merge  \\\n",
      "count                                                 161   \n",
      "unique                                                161   \n",
      "top     def remove_lb_backend_address_pool_address(cmd...   \n",
      "freq                                                    1   \n",
      "\n",
      "                                              after_merge  traceback_type  \\\n",
      "count                                                 161             161   \n",
      "unique                                                161              40   \n",
      "top     def remove_lb_backend_address_pool_address(cmd...  AttributeError   \n",
      "freq                                                    1              34   \n",
      "\n",
      "                                           full_traceback ast_diff  \\\n",
      "count                                                 161      161   \n",
      "unique                                                150      148   \n",
      "top     Ignoring exception in command guild:\\nTracebac...       []   \n",
      "freq                                                    3       14   \n",
      "\n",
      "       old_ast_json new_ast_json  \n",
      "count           161          161  \n",
      "unique          150          149  \n",
      "top              []           []  \n",
      "freq             12           13  \n",
      "/n--------------------------------------------------/n\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 161 entries, 0 to 160\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   before_merge    161 non-null    object\n",
      " 1   after_merge     161 non-null    object\n",
      " 2   traceback_type  161 non-null    object\n",
      " 3   full_traceback  161 non-null    object\n",
      " 4   ast_diff        161 non-null    object\n",
      " 5   old_ast_json    161 non-null    object\n",
      " 6   new_ast_json    161 non-null    object\n",
      "dtypes: object(7)\n",
      "memory usage: 8.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open('test_data_postprocessed.pkl', 'rb') as handle:\n",
    "    df = pd.read_pickle(handle)  # Uses Pandas' built-in method\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "print(df.head())\n",
    "print(\"/n\" + \"-\" * 50 + \"/n\")\n",
    "print(df.describe())\n",
    "print(\"/n\" + \"-\" * 50 + \"/n\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All records processed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Checking each record in the DataFrame so there are no errors\n",
    "\n",
    "for i, record in enumerate(df.to_dict(orient='records')):\n",
    "    try:\n",
    "        assert \"before_merge\" in record\n",
    "        assert \"after_merge\" in record\n",
    "        assert \"traceback_type\" in record\n",
    "        assert \"full_traceback\" in record\n",
    "        assert \"ast_diff\" in record\n",
    "        assert \"old_ast_json\" in record\n",
    "        assert \"new_ast_json\" in record\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing record {i}: {e}\")\n",
    "        print(f\"Record: {record}\")\n",
    "        break\n",
    "print(\"All records processed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No null values in the DataFrame!\n"
     ]
    }
   ],
   "source": [
    "# Checking that there are no null values in the DataFrame\n",
    "assert not df.isnull().values.any()\n",
    "print(\"No null values in the DataFrame!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checks on valid pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        before_merge  \\\n",
      "0  def hough_line_peaks(hspace, angles, dists, mi...   \n",
      "1  def find_contours(array, level,\\n             ...   \n",
      "2  def _assemble_contours(points_iterator):\\n    ...   \n",
      "3  def file_or_url_context(resource_name):\\n    \"...   \n",
      "4  def file_or_url_context(resource_name):\\n    \"...   \n",
      "\n",
      "                                         after_merge traceback_type  \\\n",
      "0  def hough_line_peaks(hspace, angles, dists, mi...     IndexError   \n",
      "1  def find_contours(array, level,\\n             ...       KeyError   \n",
      "2  def _assemble_contours(segments):\\n    current...       KeyError   \n",
      "3  def file_or_url_context(resource_name):\\n    \"...        OSError   \n",
      "4  def file_or_url_context(resource_name):\\n    \"...        OSError   \n",
      "\n",
      "                                      full_traceback  \\\n",
      "0  Traceback (most recent call last):\\nFile \"<ipy...   \n",
      "1  ----------------------------------------------...   \n",
      "2  ----------------------------------------------...   \n",
      "3  Traceback (most recent call last):\\nFile \"C:\\P...   \n",
      "4  im = imread('https://c1.staticflickr.com/9/837...   \n",
      "\n",
      "                                            ast_diff  \\\n",
      "0  [--- , +++ , @@ -1,8 +1,9 @@, -Module(body=[Fu...   \n",
      "1  [--- , +++ , @@ -1,14 +1,14 @@, -Module(body=[...   \n",
      "2  [--- , +++ , @@ -1,13 +1,13 @@, -Module(body=[...   \n",
      "3  [--- , +++ , @@ -1,14 +1,14 @@, -Module(body=[...   \n",
      "4  [--- , +++ , @@ -1,39 +1,48 @@, -Module(body=[...   \n",
      "\n",
      "                                        old_ast_json  \\\n",
      "0  [\\n    \"Module(body=[FunctionDef(name='hough_l...   \n",
      "1  [\\n    \"Module(body=[FunctionDef(name='find_co...   \n",
      "2  [\\n    \"Module(body=[FunctionDef(name='_assemb...   \n",
      "3  [\\n    \"Module(body=[FunctionDef(name='file_or...   \n",
      "4  [\\n    \"Module(body=[FunctionDef(name='file_or...   \n",
      "\n",
      "                                        new_ast_json  \n",
      "0  [\\n    \"Module(body=[FunctionDef(name='hough_l...  \n",
      "1  [\\n    \"Module(body=[FunctionDef(name='find_co...  \n",
      "2  [\\n    \"Module(body=[FunctionDef(name='_assemb...  \n",
      "3  [\\n    \"Module(body=[FunctionDef(name='file_or...  \n",
      "4  [\\n    \"Module(body=[FunctionDef(name='file_or...  \n",
      "/n--------------------------------------------------/n\n",
      "                                             before_merge  \\\n",
      "count                                                9457   \n",
      "unique                                               9457   \n",
      "top     def hough_line_peaks(hspace, angles, dists, mi...   \n",
      "freq                                                    1   \n",
      "\n",
      "                                              after_merge  traceback_type  \\\n",
      "count                                                9457            9457   \n",
      "unique                                               9431             374   \n",
      "top             def __init__(self, _io, _parent=None, ...  AttributeError   \n",
      "freq                                                    4            1603   \n",
      "\n",
      "                                           full_traceback ast_diff  \\\n",
      "count                                                9457     9457   \n",
      "unique                                               3454     9324   \n",
      "top     Traceback (most recent call last):\\nFile \"/hom...       []   \n",
      "freq                                                  206      113   \n",
      "\n",
      "                                             old_ast_json  \\\n",
      "count                                                9457   \n",
      "unique                                               9430   \n",
      "top     [\\n    \"Module(body=[FunctionDef(name='draw', ...   \n",
      "freq                                                    2   \n",
      "\n",
      "                                             new_ast_json  \n",
      "count                                                9457  \n",
      "unique                                               9401  \n",
      "top     [\\n    \"Module(body=[FunctionDef(name='__init_...  \n",
      "freq                                                    4  \n",
      "/n--------------------------------------------------/n\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9457 entries, 0 to 9456\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   before_merge    9457 non-null   object\n",
      " 1   after_merge     9457 non-null   object\n",
      " 2   traceback_type  9457 non-null   object\n",
      " 3   full_traceback  9457 non-null   object\n",
      " 4   ast_diff        9457 non-null   object\n",
      " 5   old_ast_json    9457 non-null   object\n",
      " 6   new_ast_json    9457 non-null   object\n",
      "dtypes: object(7)\n",
      "memory usage: 517.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open('valid_data_postprocessed.pkl', 'rb') as handle:\n",
    "    df = pd.read_pickle(handle)  # Uses Pandas' built-in method\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "print(df.head())\n",
    "print(\"/n\" + \"-\" * 50 + \"/n\")\n",
    "print(df.describe())\n",
    "print(\"/n\" + \"-\" * 50 + \"/n\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All records processed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Checking each record in the DataFrame so there are no errors\n",
    "\n",
    "for i, record in enumerate(df.to_dict(orient='records')):\n",
    "    try:\n",
    "        assert \"before_merge\" in record\n",
    "        assert \"after_merge\" in record\n",
    "        assert \"traceback_type\" in record\n",
    "        assert \"full_traceback\" in record\n",
    "        assert \"ast_diff\" in record\n",
    "        assert \"old_ast_json\" in record\n",
    "        assert \"new_ast_json\" in record\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing record {i}: {e}\")\n",
    "        print(f\"Record: {record}\")\n",
    "        break\n",
    "print(\"All records processed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No null values in the DataFrame!\n"
     ]
    }
   ],
   "source": [
    "# Checking that there are no null values in the DataFrame\n",
    "assert not df.isnull().values.any()\n",
    "print(\"No null values in the DataFrame!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
