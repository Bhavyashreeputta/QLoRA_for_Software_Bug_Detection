{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             after_merge  \\\n",
      "1790   def plot(result_pickle_file_path, show, plot_s...   \n",
      "2433       def stream_logs(self):\\n        \"\"\"Stream ...   \n",
      "26618      def addRecentProjectFile(self, projectFile...   \n",
      "26622      def addSfmAugmentation(self, withMVS=False...   \n",
      "28217      def load_pymathics_doc(self):\\n        if ...   \n",
      "\n",
      "                                            before_merge  \\\n",
      "1790   def plot(result_dict_file, show, plot_save_fil...   \n",
      "2433       def stream_logs(self):\\n        \"\"\"Stream ...   \n",
      "26618      def addRecentProjectFile(self, projectFile...   \n",
      "26622      def addSfmAugmentation(self, withMVS=False...   \n",
      "28217      def load_pymathics_doc(self):\\n        if ...   \n",
      "\n",
      "                                               filename  \\\n",
      "1790   rqalpha/mod/rqalpha_mod_sys_analyser/__init__.py   \n",
      "2433                                 binderhub/build.py   \n",
      "26618                                meshroom/ui/app.py   \n",
      "26622                     meshroom/ui/reconstruction.py   \n",
      "28217                                mathics/doc/doc.py   \n",
      "\n",
      "                              full_file_code_after_merge  \\\n",
      "1790   # -*- coding: utf-8 -*-\\n#\\n# Copyright 2017 R...   \n",
      "2433   \"\"\"\\nContains build of a docker image from a g...   \n",
      "26618  import logging\\nimport os\\nimport argparse\\n\\n...   \n",
      "26622  import logging\\nimport os\\nfrom threading impo...   \n",
      "28217  #!/usr/bin/env python3\\n# -*- coding: utf-8 -*...   \n",
      "\n",
      "                             full_file_code_before_merge  \\\n",
      "1790   # -*- coding: utf-8 -*-\\n#\\n# Copyright 2017 R...   \n",
      "2433   \"\"\"\\nContains build of a docker image from a g...   \n",
      "26618  import logging\\nimport os\\nimport argparse\\n\\n...   \n",
      "26622  import logging\\nimport os\\nfrom threading impo...   \n",
      "28217  #!/usr/bin/env python3\\n# -*- coding: utf-8 -*...   \n",
      "\n",
      "                                     function_name  \\\n",
      "1790                                          plot   \n",
      "2433                             Build.stream_logs   \n",
      "26618             MeshroomApp.addRecentProjectFile   \n",
      "26622            Reconstruction.addSfmAugmentation   \n",
      "28217  MathicsMainDocumentation.load_pymathics_doc   \n",
      "\n",
      "                                                     url  \\\n",
      "1790     https://github.com/ricequant/rqalpha/issues/109   \n",
      "2433   https://github.com/jupyterhub/binderhub/issues...   \n",
      "26618  https://github.com/alicevision/meshroom/issues...   \n",
      "26622  https://github.com/alicevision/meshroom/issues...   \n",
      "28217      https://github.com/mathics/Mathics/issues/906   \n",
      "\n",
      "                                  source code and errors  \\\n",
      "1790   [{'piece_type': 'other', 'piece_content': 'rqa...   \n",
      "2433   [{'piece_type': 'error message', 'piece_conten...   \n",
      "26618  [{'piece_type': 'error message', 'piece_conten...   \n",
      "26622  [{'piece_type': 'error message', 'piece_conten...   \n",
      "28217  [{'piece_type': 'error message', 'piece_conten...   \n",
      "\n",
      "                                          full_traceback     traceback_type  \\\n",
      "1790   Traceback (most recent call last):\\nFile \"c:\\p...          TypeError   \n",
      "2433   / # jupyter-repo2docker https://github.com/yuv...  FileNotFoundError   \n",
      "26618  [2020-05-23 16:12:48,660][ERROR] Traceback (mo...            OSError   \n",
      "26622  Traceback (most recent call last):\\nFile \"C:\\U...       RuntimeError   \n",
      "28217  $ mathicsserver\\nwarning: database file /home/...           KeyError   \n",
      "\n",
      "                         before_merge_without_docstrings  \\\n",
      "1790   def plot(result_dict_file, show, plot_save_fil...   \n",
      "2433       def stream_logs(self):\\n        \\n        ...   \n",
      "26618      def addRecentProjectFile(self, projectFile...   \n",
      "26622      def addSfmAugmentation(self, withMVS=False...   \n",
      "28217      def load_pymathics_doc(self):\\n        if ...   \n",
      "\n",
      "                          after_merge_without_docstrings  \\\n",
      "1790   def plot(result_pickle_file_path, show, plot_s...   \n",
      "2433       def stream_logs(self):\\n        \\n        ...   \n",
      "26618      def addRecentProjectFile(self, projectFile...   \n",
      "26622      def addSfmAugmentation(self, withMVS=False...   \n",
      "28217      def load_pymathics_doc(self):\\n        if ...   \n",
      "\n",
      "                      before_merge_docstrings  \\\n",
      "1790   [[sys_analyser] draw result DataFrame]   \n",
      "2433                                       []   \n",
      "26618                                      []   \n",
      "26622                                      []   \n",
      "28217                                      []   \n",
      "\n",
      "                       after_merge_docstrings  \\\n",
      "1790   [[sys_analyser] draw result DataFrame]   \n",
      "2433                                       []   \n",
      "26618                                      []   \n",
      "26622                                      []   \n",
      "28217                                      []   \n",
      "\n",
      "                            path_to_snippet_before_merge  \\\n",
      "1790   buggy_snippets_files/e93817735d3042d739fe86677...   \n",
      "2433   buggy_snippets_files/8241189c4267b81254c9ed07a...   \n",
      "26618  buggy_snippets_files/faddf4c059bd32cc1cad1a1ea...   \n",
      "26622  buggy_snippets_files/dffb9602005cbea45f7d0c6d2...   \n",
      "28217  buggy_snippets_files/c98cf1a03e1d7e716b228fbe8...   \n",
      "\n",
      "                             path_to_snippet_after_merge  \n",
      "1790   buggy_snippets_files/e93817735d3042d739fe86677...  \n",
      "2433   buggy_snippets_files/8241189c4267b81254c9ed07a...  \n",
      "26618  buggy_snippets_files/faddf4c059bd32cc1cad1a1ea...  \n",
      "26622  buggy_snippets_files/dffb9602005cbea45f7d0c6d2...  \n",
      "28217  buggy_snippets_files/c98cf1a03e1d7e716b228fbe8...  \n"
     ]
    }
   ],
   "source": [
    "#opening the pickle file to view the dataframe\n",
    "import pandas as pd\n",
    "\n",
    "with open('buggy_dataset/bugfixes_train.pickle', 'rb') as handle:\n",
    "    df = pd.read_pickle(handle)  # Uses Pandas' built-in method\n",
    "\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14118, 16)\n",
      "Index(['after_merge', 'before_merge', 'filename', 'full_file_code_after_merge',\n",
      "       'full_file_code_before_merge', 'function_name', 'url',\n",
      "       'source code and errors', 'full_traceback', 'traceback_type',\n",
      "       'before_merge_without_docstrings', 'after_merge_without_docstrings',\n",
      "       'before_merge_docstrings', 'after_merge_docstrings',\n",
      "       'path_to_snippet_before_merge', 'path_to_snippet_after_merge'],\n",
      "      dtype='object')\n",
      "after_merge                        object\n",
      "before_merge                       object\n",
      "filename                           object\n",
      "full_file_code_after_merge         object\n",
      "full_file_code_before_merge        object\n",
      "function_name                      object\n",
      "url                                object\n",
      "source code and errors             object\n",
      "full_traceback                     object\n",
      "traceback_type                     object\n",
      "before_merge_without_docstrings    object\n",
      "after_merge_without_docstrings     object\n",
      "before_merge_docstrings            object\n",
      "after_merge_docstrings             object\n",
      "path_to_snippet_before_merge       object\n",
      "path_to_snippet_after_merge        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              after_merge  \\\n",
      "count                                               14118   \n",
      "unique                                              14055   \n",
      "top         def search_novel(self, query):\\n        qu...   \n",
      "freq                                                    4   \n",
      "\n",
      "                                             before_merge         filename  \\\n",
      "count                                               14118            14118   \n",
      "unique                                              14118             5352   \n",
      "top     def plot(result_dict_file, show, plot_save_fil...  mypy/semanal.py   \n",
      "freq                                                    1               63   \n",
      "\n",
      "                               full_file_code_after_merge  \\\n",
      "count                                               14118   \n",
      "unique                                               8074   \n",
      "top     #!/usr/bin/env python\\n# -*- coding: utf-8 -*-...   \n",
      "freq                                                   47   \n",
      "\n",
      "                              full_file_code_before_merge function_name  \\\n",
      "count                                               14118         14118   \n",
      "unique                                               8070         12155   \n",
      "top     #!/usr/bin/env python\\n# -*- coding: utf-8 -*-...          main   \n",
      "freq                                                   47           125   \n",
      "\n",
      "                                               url  \\\n",
      "count                                        14118   \n",
      "unique                                        4693   \n",
      "top     https://github.com/numba/numba/issues/4944   \n",
      "freq                                           231   \n",
      "\n",
      "                                   source code and errors  \\\n",
      "count                                               14118   \n",
      "unique                                               4693   \n",
      "top     [{'piece_type': 'other', 'piece_content': 'cla...   \n",
      "freq                                                  231   \n",
      "\n",
      "                                           full_traceback  traceback_type  \\\n",
      "count                                               14118           14118   \n",
      "unique                                               4693             555   \n",
      "top     Traceback (most recent call last):\\nFile \"/hom...  AttributeError   \n",
      "freq                                                  231            2378   \n",
      "\n",
      "                          before_merge_without_docstrings  \\\n",
      "count                                               14118   \n",
      "unique                                              14103   \n",
      "top         def to_dict(self, orient='dict', into=dict...   \n",
      "freq                                                    2   \n",
      "\n",
      "                           after_merge_without_docstrings  \\\n",
      "count                                               14118   \n",
      "unique                                              14039   \n",
      "top         def search_novel(self, query):\\n        qu...   \n",
      "freq                                                    4   \n",
      "\n",
      "       before_merge_docstrings after_merge_docstrings  \\\n",
      "count                    14118                  14118   \n",
      "unique                    2090                   2097   \n",
      "top                         []                     []   \n",
      "freq                     11837                  11822   \n",
      "\n",
      "                             path_to_snippet_before_merge  \\\n",
      "count                                               14118   \n",
      "unique                                              14118   \n",
      "top     buggy_snippets_files/e93817735d3042d739fe86677...   \n",
      "freq                                                    1   \n",
      "\n",
      "                              path_to_snippet_after_merge  \n",
      "count                                               14118  \n",
      "unique                                              14118  \n",
      "top     buggy_snippets_files/e93817735d3042d739fe86677...  \n",
      "freq                                                    1  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14118 entries, 1790 to 625636\n",
      "Data columns (total 16 columns):\n",
      " #   Column                           Non-Null Count  Dtype \n",
      "---  ------                           --------------  ----- \n",
      " 0   after_merge                      14118 non-null  object\n",
      " 1   before_merge                     14118 non-null  object\n",
      " 2   filename                         14118 non-null  object\n",
      " 3   full_file_code_after_merge       14118 non-null  object\n",
      " 4   full_file_code_before_merge      14118 non-null  object\n",
      " 5   function_name                    14118 non-null  object\n",
      " 6   url                              14118 non-null  object\n",
      " 7   source code and errors           14118 non-null  object\n",
      " 8   full_traceback                   14118 non-null  object\n",
      " 9   traceback_type                   14118 non-null  object\n",
      " 10  before_merge_without_docstrings  14118 non-null  object\n",
      " 11  after_merge_without_docstrings   14118 non-null  object\n",
      " 12  before_merge_docstrings          14118 non-null  object\n",
      " 13  after_merge_docstrings           14118 non-null  object\n",
      " 14  path_to_snippet_before_merge     14118 non-null  object\n",
      " 15  path_to_snippet_after_merge      14118 non-null  object\n",
      "dtypes: object(16)\n",
      "memory usage: 1.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.describe())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before merge column\n",
      "def plot(result_dict_file, show, plot_save_file):\n",
      "    \"\"\"\n",
      "    [sys_analyser] draw result DataFrame\n",
      "    \"\"\"\n",
      "    import pandas as pd\n",
      "    from .plot import plot_result\n",
      "\n",
      "    result_dict = pd.read_pickle(result_dict_file)\n",
      "    plot_result(result_dict, show, plot_save_file)\n",
      "--------------------\n",
      "after merge column\n",
      "def plot(result_pickle_file_path, show, plot_save_file):\n",
      "    \"\"\"\n",
      "    [sys_analyser] draw result DataFrame\n",
      "    \"\"\"\n",
      "    import pandas as pd\n",
      "    from .plot import plot_result\n",
      "\n",
      "    result_dict = pd.read_pickle(result_pickle_file_path)\n",
      "    plot_result(result_dict, show, plot_save_file)\n",
      "--------------------\n",
      "traceback type column\n",
      "TypeError\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "df1 = df[[\"before_merge\", \"after_merge\", \"traceback_type\", \"full_traceback\", \"source code and errors\", \"function_name\", \"before_merge_docstrings\", \"after_merge_docstrings\"]]\n",
    "df2 = df1.head()\n",
    "print(\"before merge column\")\n",
    "print(df2.iloc[0][\"before_merge\"])\n",
    "print(\"-\"*20)\n",
    "print(\"after merge column\")\n",
    "print(df2.iloc[0][\"after_merge\"])\n",
    "print(\"-\"*20)\n",
    "print(\"traceback type column\")\n",
    "print(df2.iloc[0][\"traceback_type\"])\n",
    "print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full traceback column\n",
      "/ # jupyter-repo2docker https://github.com/yuvipanda/example-requirements --json-logs\n",
      "Traceback (most recent call last):\n",
      "File \"/usr/local/bin/jupyter-repo2docker\", line 11, in <module>\n",
      "load_entry_point('jupyter-repo2docker==0.4.1', 'console_scripts', 'jupyter-repo2docker')()\n",
      "File \"/usr/local/lib/python3.6/site-packages/repo2docker/__main__.py\", line 6, in main\n",
      "f.start()\n",
      "File \"/usr/local/lib/python3.6/site-packages/repo2docker/app.py\", line 309, in start\n",
      "checkout_path\n",
      "File \"/usr/local/lib/python3.6/site-packages/repo2docker/app.py\", line 95, in fetch\n",
      "capture=self.json_logs):\n",
      "File \"/usr/local/lib/python3.6/site-packages/repo2docker/utils.py\", line 12, in execute_cmd\n",
      "proc = subprocess.Popen(cmd, **kwargs)\n",
      "File \"/usr/local/lib/python3.6/subprocess.py\", line 709, in __init__\n",
      "restore_signals, start_new_session)\n",
      "File \"/usr/local/lib/python3.6/subprocess.py\", line 1344, in _execute_child\n",
      "raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'git': 'git'\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"full traceback column\")\n",
    "print(df2.iloc[1][\"full_traceback\"])\n",
    "print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source code and errors column\n",
      "[{'piece_type': 'error message', 'piece_content': '[2020-05-23 16:12:48,660][ERROR] Traceback (most recent call last):\\nFile \"D:\\\\Meshroom_Src\\\\meshroom\\\\meshroom\\\\ui\\\\reconstruction.py\", line 432, in load\\nsuper(Reconstruction, self).load(filepath, setupProjectFile)\\nFile \"D:\\\\Meshroom_Src\\\\meshroom\\\\meshroom\\\\ui\\\\graph.py\", line 314, in load\\ng.load(filepath, setupProjectFile)\\nFile \"D:\\\\Meshroom_Src\\\\meshroom\\\\meshroom\\\\core\\\\graph.py\", line 247, in load\\nwith open(filepath) as jsonFile:\\nOSError: [Errno 22] Invalid argument: \\'/D:/Meshroom_Dev/test-project/mostree.mg\\''}]\n",
      "--------------------\n",
      "function name column\n",
      "Reconstruction.addSfmAugmentation\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"source code and errors column\")\n",
    "print(df2.iloc[2][\"source code and errors\"])\n",
    "print(\"-\"*20)\n",
    "print(\"function name column\")\n",
    "print(df2.iloc[3][\"function_name\"])\n",
    "print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before merge docstrings column\n",
      "[]\n",
      "--------------------\n",
      "after merge docstrings column\n",
      "[]\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"before merge docstrings column\")\n",
    "print(df2.iloc[4][\"before_merge_docstrings\"])\n",
    "print(\"-\"*20)\n",
    "print(\"after merge docstrings column\")\n",
    "print(df2.iloc[4][\"after_merge_docstrings\"])\n",
    "print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Index: 5 entries, 1790 to 28217\n",
      "Series name: before_merge\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "5 non-null      object\n",
      "dtypes: object(1)\n",
      "memory usage: 80.0+ bytes\n",
      "None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "count                                                     5\n",
      "unique                                                    5\n",
      "top       def plot(result_dict_file, show, plot_save_fil...\n",
      "freq                                                      1\n",
      "Name: before_merge, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df2[\"before_merge\"].info())\n",
    "print(\"-\"*100)\n",
    "print(df2[\"before_merge\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(df2[\"before_merge\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def plot(result_dict_file, show, plot_save_file):\n",
      "    \"\"\"\n",
      "    [sys_analyser] draw result DataFrame\n",
      "    \"\"\"\n",
      "    import pandas as pd\n",
      "    from .plot import plot_result\n",
      "\n",
      "    result_dict = pd.read_pickle(result_dict_file)\n",
      "    plot_result(result_dict, show, plot_save_file)\n",
      "--------------------\n",
      "    def stream_logs(self):\n",
      "        \"\"\"Stream a pod's log.\"\"\"\n",
      "        for line in self.api.read_namespaced_pod_log(\n",
      "                self.name,\n",
      "                self.namespace,\n",
      "                follow=True,\n",
      "                _preload_content=False):\n",
      "\n",
      "            self.progress('log', line.decode('utf-8'))\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print(df2[\"before_merge\"].iloc[0])\n",
    "print(\"-\"*20)\n",
    "print(df2[\"before_merge\"].iloc[1])\n",
    "print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def addRecentProjectFile(self, projectFile):\n",
      "        projectFile = QUrl(projectFile).path()\n",
      "        projects = self._recentProjectFiles()\n",
      "\n",
      "        # remove duplicates while preserving order\n",
      "        from collections import OrderedDict\n",
      "        uniqueProjects = OrderedDict.fromkeys(projects)\n",
      "        projects = list(uniqueProjects)\n",
      "        # remove previous usage of the value\n",
      "        if projectFile in uniqueProjects:\n",
      "            projects.remove(projectFile)\n",
      "        # add the new value in the first place\n",
      "        projects.insert(0, projectFile)\n",
      "\n",
      "        # keep only the 10 first elements\n",
      "        projects = projects[0:20]\n",
      "\n",
      "        settings = QSettings()\n",
      "        settings.beginGroup(\"RecentFiles\")\n",
      "        size = settings.beginWriteArray(\"Projects\")\n",
      "        for i, p in enumerate(projects):\n",
      "            settings.setArrayIndex(i)\n",
      "            settings.setValue(\"filepath\", p)\n",
      "        settings.endArray()\n",
      "        settings.sync()\n",
      "\n",
      "        self.recentProjectFilesChanged.emit()\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print(df2[\"before_merge\"].iloc[2])\n",
    "print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def addSfmAugmentation(self, withMVS=False):\n",
      "        \"\"\"\n",
      "        Create a new augmentation step connected to the last SfM node of this Reconstruction and\n",
      "        return the created CameraInit and SfM nodes.\n",
      "\n",
      "        If the Reconstruction is not initialized (empty initial CameraInit), this method won't\n",
      "        create anything and return initial CameraInit and SfM nodes.\n",
      "\n",
      "        Args:\n",
      "            withMVS (bool): whether to create the MVS pipeline after the augmentation\n",
      "\n",
      "        Returns:\n",
      "            Node, Node: CameraInit, StructureFromMotion\n",
      "        \"\"\"\n",
      "        sfm = self.lastSfmNode()\n",
      "        if not sfm:\n",
      "            return None, None\n",
      "\n",
      "        if len(self._cameraInits) == 1:\n",
      "            assert self._cameraInit == self._cameraInits[0]\n",
      "            # Initial CameraInit is empty, use this one\n",
      "            if len(self._cameraInits[0].viewpoints) == 0:\n",
      "                return self._cameraInit, sfm\n",
      "\n",
      "        with self.groupedGraphModification(\"SfM Augmentation\"):\n",
      "            sfm, mvs = multiview.sfmAugmentation(self, self.lastSfmNode(), withMVS=withMVS)\n",
      "\n",
      "        self.sfmAugmented.emit(sfm[0], mvs[-1])\n",
      "        return sfm[0], sfm[-1]\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print(df2[\"before_merge\"].iloc[3])\n",
    "print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def load_pymathics_doc(self):\n",
      "        if self.pymathics_doc_loaded:\n",
      "            return\n",
      "        from mathics.settings import default_pymathics_modules\n",
      "        pymathicspart = None\n",
      "        # Look the \"Pymathics Modules\" part, and if it does not exist, create it.\n",
      "        for part in self.parts:\n",
      "            if part.title == \"Pymathics Modules\":\n",
      "                pymathicspart = part\n",
      "        if pymathicspart is None:\n",
      "            pymathicspart = DocPart(self, \"Pymathics Modules\", is_reference=True)\n",
      "            self.parts.append(pymathicspart)\n",
      "\n",
      "        # For each module, create the documentation object and load the chapters in the pymathics part.\n",
      "        for pymmodule in default_pymathics_modules:\n",
      "            pymathicsdoc = PyMathicsDocumentation(pymmodule)\n",
      "            for part in pymathicsdoc.parts:\n",
      "                for ch in part.chapters:\n",
      "                    ch.title = f\"{pymmodule.name} {part.title} {ch.title}\"\n",
      "                    ch.part = pymathicspart\n",
      "                    pymathicspart.chapters_by_slug[ch.slug] = ch\n",
      "                    pymathicspart.chapters.append(ch)\n",
      "\n",
      "        self.pymathics_doc_loaded = True\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print(df2[\"before_merge\"].iloc[4])\n",
    "print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print(type(df2[\"before_merge\"].iloc[0]))\n",
    "print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'def plot(result_dict_file, show, plot_save_file):\\n    \"\"\"\\n    [sys_analyser] draw result DataFrame\\n    \"\"\"\\n    import pandas as pd\\n    from .plot import plot_result\\n\\n    result_dict = pd.read_pickle(result_dict_file)\\n    plot_result(result_dict, show, plot_save_file)'\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print(repr(df2[\"before_merge\"].iloc[0]))\n",
    "print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the code snippet in string into AST\n",
    "import ast\n",
    "code_before = df2[\"before_merge\"].iloc[0]\n",
    "code_after = df2[\"after_merge\"].iloc[0]\n",
    "code_before_ast = ast.parse(code_before)\n",
    "code_after_ast = ast.parse(code_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module(\n",
      "    body=[\n",
      "        FunctionDef(\n",
      "            name='plot',\n",
      "            args=arguments(\n",
      "                posonlyargs=[],\n",
      "                args=[\n",
      "                    arg(arg='result_dict_file'),\n",
      "                    arg(arg='show'),\n",
      "                    arg(arg='plot_save_file')],\n",
      "                kwonlyargs=[],\n",
      "                kw_defaults=[],\n",
      "                defaults=[]),\n",
      "            body=[\n",
      "                Expr(\n",
      "                    value=Constant(value='\\n    [sys_analyser] draw result DataFrame\\n    ')),\n",
      "                Import(\n",
      "                    names=[\n",
      "                        alias(name='pandas', asname='pd')]),\n",
      "                ImportFrom(\n",
      "                    module='plot',\n",
      "                    names=[\n",
      "                        alias(name='plot_result')],\n",
      "                    level=1),\n",
      "                Assign(\n",
      "                    targets=[\n",
      "                        Name(id='result_dict', ctx=Store())],\n",
      "                    value=Call(\n",
      "                        func=Attribute(\n",
      "                            value=Name(id='pd', ctx=Load()),\n",
      "                            attr='read_pickle',\n",
      "                            ctx=Load()),\n",
      "                        args=[\n",
      "                            Name(id='result_dict_file', ctx=Load())],\n",
      "                        keywords=[])),\n",
      "                Expr(\n",
      "                    value=Call(\n",
      "                        func=Name(id='plot_result', ctx=Load()),\n",
      "                        args=[\n",
      "                            Name(id='result_dict', ctx=Load()),\n",
      "                            Name(id='show', ctx=Load()),\n",
      "                            Name(id='plot_save_file', ctx=Load())],\n",
      "                        keywords=[]))],\n",
      "            decorator_list=[],\n",
      "            type_params=[])],\n",
      "    type_ignores=[])\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print(ast.dump(code_before_ast, indent=4))\n",
    "print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module(\n",
      "    body=[\n",
      "        FunctionDef(\n",
      "            name='plot',\n",
      "            args=arguments(\n",
      "                posonlyargs=[],\n",
      "                args=[\n",
      "                    arg(arg='result_pickle_file_path'),\n",
      "                    arg(arg='show'),\n",
      "                    arg(arg='plot_save_file')],\n",
      "                kwonlyargs=[],\n",
      "                kw_defaults=[],\n",
      "                defaults=[]),\n",
      "            body=[\n",
      "                Expr(\n",
      "                    value=Constant(value='\\n    [sys_analyser] draw result DataFrame\\n    ')),\n",
      "                Import(\n",
      "                    names=[\n",
      "                        alias(name='pandas', asname='pd')]),\n",
      "                ImportFrom(\n",
      "                    module='plot',\n",
      "                    names=[\n",
      "                        alias(name='plot_result')],\n",
      "                    level=1),\n",
      "                Assign(\n",
      "                    targets=[\n",
      "                        Name(id='result_dict', ctx=Store())],\n",
      "                    value=Call(\n",
      "                        func=Attribute(\n",
      "                            value=Name(id='pd', ctx=Load()),\n",
      "                            attr='read_pickle',\n",
      "                            ctx=Load()),\n",
      "                        args=[\n",
      "                            Name(id='result_pickle_file_path', ctx=Load())],\n",
      "                        keywords=[])),\n",
      "                Expr(\n",
      "                    value=Call(\n",
      "                        func=Name(id='plot_result', ctx=Load()),\n",
      "                        args=[\n",
      "                            Name(id='result_dict', ctx=Load()),\n",
      "                            Name(id='show', ctx=Load()),\n",
      "                            Name(id='plot_save_file', ctx=Load())],\n",
      "                        keywords=[]))],\n",
      "            decorator_list=[],\n",
      "            type_params=[])],\n",
      "    type_ignores=[])\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print(ast.dump(code_after_ast, indent=4))\n",
    "print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FunctionVisitor(ast.NodeVisitor):\n",
    "    def visit_FunctionDef(self, node):\n",
    "        print(f\"Function name: {node.name}\")\n",
    "        print(f\"Arguments: {[arg.arg for arg in node.args.args]}\")\n",
    "        self.generic_visit(node)\n",
    "\n",
    "visitor = FunctionVisitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function name: plot\n",
      "Arguments: ['result_dict_file', 'show', 'plot_save_file']\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "visitor.visit(code_before_ast)\n",
    "print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function name: plot\n",
      "Arguments: ['result_pickle_file_path', 'show', 'plot_save_file']\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "visitor.visit(code_after_ast)\n",
    "print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- \n",
      "+++ \n",
      "@@ -1,9 +1,9 @@\n",
      "-def plot(result_dict_file, show, plot_save_file):\n",
      "+def plot(result_pickle_file_path, show, plot_save_file):\n",
      "     \"\"\"\n",
      "     [sys_analyser] draw result DataFrame\n",
      "     \"\"\"\n",
      "     import pandas as pd\n",
      "     from .plot import plot_result\n",
      " \n",
      "-    result_dict = pd.read_pickle(result_dict_file)\n",
      "+    result_dict = pd.read_pickle(result_pickle_file_path)\n",
      "     plot_result(result_dict, show, plot_save_file)\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "diff = difflib.unified_diff(code_before.splitlines(), code_after.splitlines(), lineterm='')\n",
    "print('\\n'.join(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    now i know the diff lines and type of traceback, i can use this information to train a model to predict the bugfixes\\n    \\n        i will use the before_merge and after_merge columns to train the llm\\n        i will use the traceback_type column as the target variable\\n\\n    how will i do that?\\n\\n        i will first convert the code snippets into AST\\n        then i will extract the function name and arguments from the AST\\n        then i will extract the diff lines\\n        then i will extract the traceback type\\n        then i will use the above information to train the model\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    now i know the diff lines and type of traceback, i can use this information to train a model to predict the bugfixes\n",
    "    \n",
    "        i will use the before_merge and after_merge columns to train the llm\n",
    "        i will use the traceback_type column as the target variable\n",
    "\n",
    "    how will i do that?\n",
    "\n",
    "        i will first convert the code snippets into AST\n",
    "        then i will extract the function name and arguments from the AST\n",
    "        then i will extract the diff lines\n",
    "        then i will extract the traceback type\n",
    "        then i will use the above information to train the model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n"
     ]
    }
   ],
   "source": [
    "print(type(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ast_nodes(code):\n",
    "    \"\"\"Parse Python code and return AST nodes.\"\"\"\n",
    "    tree = ast.parse(code)\n",
    "    return [node for node in ast.walk(tree)]  # Walk through all AST nodes\n",
    "\n",
    "def extract_changed_nodes(old_code, new_code):\n",
    "    \"\"\"Find changed AST nodes by comparing old and new code.\"\"\"\n",
    "    old_ast_nodes = get_ast_nodes(old_code)\n",
    "    new_ast_nodes = get_ast_nodes(new_code)\n",
    "\n",
    "    # Convert AST nodes to string representation\n",
    "    old_ast_str = [ast.dump(node) for node in old_ast_nodes]\n",
    "    new_ast_str = [ast.dump(node) for node in new_ast_nodes]\n",
    "\n",
    "    # Compute differences at the AST level\n",
    "    diff = list(difflib.unified_diff(old_ast_str, new_ast_str, lineterm=\"\"))\n",
    "\n",
    "    return diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- \n",
      "+++ \n",
      "@@ -1,23 +1,23 @@\n",
      "-Module(body=[FunctionDef(name='plot', args=arguments(posonlyargs=[], args=[arg(arg='result_dict_file'), arg(arg='show'), arg(arg='plot_save_file')], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value='\\n    [sys_analyser] draw result DataFrame\\n    ')), Import(names=[alias(name='pandas', asname='pd')]), ImportFrom(module='plot', names=[alias(name='plot_result')], level=1), Assign(targets=[Name(id='result_dict', ctx=Store())], value=Call(func=Attribute(value=Name(id='pd', ctx=Load()), attr='read_pickle', ctx=Load()), args=[Name(id='result_dict_file', ctx=Load())], keywords=[])), Expr(value=Call(func=Name(id='plot_result', ctx=Load()), args=[Name(id='result_dict', ctx=Load()), Name(id='show', ctx=Load()), Name(id='plot_save_file', ctx=Load())], keywords=[]))], decorator_list=[], type_params=[])], type_ignores=[])\n",
      "-FunctionDef(name='plot', args=arguments(posonlyargs=[], args=[arg(arg='result_dict_file'), arg(arg='show'), arg(arg='plot_save_file')], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value='\\n    [sys_analyser] draw result DataFrame\\n    ')), Import(names=[alias(name='pandas', asname='pd')]), ImportFrom(module='plot', names=[alias(name='plot_result')], level=1), Assign(targets=[Name(id='result_dict', ctx=Store())], value=Call(func=Attribute(value=Name(id='pd', ctx=Load()), attr='read_pickle', ctx=Load()), args=[Name(id='result_dict_file', ctx=Load())], keywords=[])), Expr(value=Call(func=Name(id='plot_result', ctx=Load()), args=[Name(id='result_dict', ctx=Load()), Name(id='show', ctx=Load()), Name(id='plot_save_file', ctx=Load())], keywords=[]))], decorator_list=[], type_params=[])\n",
      "-arguments(posonlyargs=[], args=[arg(arg='result_dict_file'), arg(arg='show'), arg(arg='plot_save_file')], kwonlyargs=[], kw_defaults=[], defaults=[])\n",
      "+Module(body=[FunctionDef(name='plot', args=arguments(posonlyargs=[], args=[arg(arg='result_pickle_file_path'), arg(arg='show'), arg(arg='plot_save_file')], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value='\\n    [sys_analyser] draw result DataFrame\\n    ')), Import(names=[alias(name='pandas', asname='pd')]), ImportFrom(module='plot', names=[alias(name='plot_result')], level=1), Assign(targets=[Name(id='result_dict', ctx=Store())], value=Call(func=Attribute(value=Name(id='pd', ctx=Load()), attr='read_pickle', ctx=Load()), args=[Name(id='result_pickle_file_path', ctx=Load())], keywords=[])), Expr(value=Call(func=Name(id='plot_result', ctx=Load()), args=[Name(id='result_dict', ctx=Load()), Name(id='show', ctx=Load()), Name(id='plot_save_file', ctx=Load())], keywords=[]))], decorator_list=[], type_params=[])], type_ignores=[])\n",
      "+FunctionDef(name='plot', args=arguments(posonlyargs=[], args=[arg(arg='result_pickle_file_path'), arg(arg='show'), arg(arg='plot_save_file')], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value='\\n    [sys_analyser] draw result DataFrame\\n    ')), Import(names=[alias(name='pandas', asname='pd')]), ImportFrom(module='plot', names=[alias(name='plot_result')], level=1), Assign(targets=[Name(id='result_dict', ctx=Store())], value=Call(func=Attribute(value=Name(id='pd', ctx=Load()), attr='read_pickle', ctx=Load()), args=[Name(id='result_pickle_file_path', ctx=Load())], keywords=[])), Expr(value=Call(func=Name(id='plot_result', ctx=Load()), args=[Name(id='result_dict', ctx=Load()), Name(id='show', ctx=Load()), Name(id='plot_save_file', ctx=Load())], keywords=[]))], decorator_list=[], type_params=[])\n",
      "+arguments(posonlyargs=[], args=[arg(arg='result_pickle_file_path'), arg(arg='show'), arg(arg='plot_save_file')], kwonlyargs=[], kw_defaults=[], defaults=[])\n",
      " Expr(value=Constant(value='\\n    [sys_analyser] draw result DataFrame\\n    '))\n",
      " Import(names=[alias(name='pandas', asname='pd')])\n",
      " ImportFrom(module='plot', names=[alias(name='plot_result')], level=1)\n",
      "-Assign(targets=[Name(id='result_dict', ctx=Store())], value=Call(func=Attribute(value=Name(id='pd', ctx=Load()), attr='read_pickle', ctx=Load()), args=[Name(id='result_dict_file', ctx=Load())], keywords=[]))\n",
      "+Assign(targets=[Name(id='result_dict', ctx=Store())], value=Call(func=Attribute(value=Name(id='pd', ctx=Load()), attr='read_pickle', ctx=Load()), args=[Name(id='result_pickle_file_path', ctx=Load())], keywords=[]))\n",
      " Expr(value=Call(func=Name(id='plot_result', ctx=Load()), args=[Name(id='result_dict', ctx=Load()), Name(id='show', ctx=Load()), Name(id='plot_save_file', ctx=Load())], keywords=[]))\n",
      "-arg(arg='result_dict_file')\n",
      "+arg(arg='result_pickle_file_path')\n",
      " arg(arg='show')\n",
      " arg(arg='plot_save_file')\n",
      " Constant(value='\\n    [sys_analyser] draw result DataFrame\\n    ')\n",
      " alias(name='pandas', asname='pd')\n",
      " alias(name='plot_result')\n",
      " Name(id='result_dict', ctx=Store())\n",
      "-Call(func=Attribute(value=Name(id='pd', ctx=Load()), attr='read_pickle', ctx=Load()), args=[Name(id='result_dict_file', ctx=Load())], keywords=[])\n",
      "+Call(func=Attribute(value=Name(id='pd', ctx=Load()), attr='read_pickle', ctx=Load()), args=[Name(id='result_pickle_file_path', ctx=Load())], keywords=[])\n",
      " Call(func=Name(id='plot_result', ctx=Load()), args=[Name(id='result_dict', ctx=Load()), Name(id='show', ctx=Load()), Name(id='plot_save_file', ctx=Load())], keywords=[])\n",
      " Store()\n",
      " Attribute(value=Name(id='pd', ctx=Load()), attr='read_pickle', ctx=Load())\n",
      "-Name(id='result_dict_file', ctx=Load())\n",
      "+Name(id='result_pickle_file_path', ctx=Load())\n",
      " Name(id='plot_result', ctx=Load())\n",
      " Name(id='result_dict', ctx=Load())\n",
      " Name(id='show', ctx=Load())\n"
     ]
    }
   ],
   "source": [
    "diff_output = extract_changed_nodes(code_before, code_after)\n",
    "print(\"\\n\".join(diff_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_ast_json(code):\n",
    "    \"\"\"Convert Python code to an AST representation in JSON format.\"\"\"\n",
    "    tree = ast.parse(code)\n",
    "    nodes = [ast.dump(node) for node in ast.walk(tree)]\n",
    "    return nodes  # Return as a list of AST node strings\n",
    "\n",
    "def generate_ast_diff_json(old_code, new_code):\n",
    "    \"\"\"Generate a structured JSON diff between old and new ASTs.\"\"\"\n",
    "    old_ast = get_ast_json(old_code)\n",
    "    new_ast = get_ast_json(new_code)\n",
    "\n",
    "    # Compute AST differences\n",
    "    diff = list(difflib.unified_diff(old_ast, new_ast, lineterm=\"\"))\n",
    "\n",
    "    # Structure the diff in JSON format\n",
    "    diff_json = {\n",
    "        \"buggy_code\": old_code,\n",
    "        \"fixed_code\": new_code,\n",
    "        \"ast_diff\": diff,\n",
    "        \"traceback_type\": \"TypeError\", # Placeholder for actual traceback type\n",
    "    }\n",
    "\n",
    "    return json.dumps(diff_json, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"buggy_code\": \"def plot(result_dict_file, show, plot_save_file):\\n    \\\"\\\"\\\"\\n    [sys_analyser] draw result DataFrame\\n    \\\"\\\"\\\"\\n    import pandas as pd\\n    from .plot import plot_result\\n\\n    result_dict = pd.read_pickle(result_dict_file)\\n    plot_result(result_dict, show, plot_save_file)\",\n",
      "    \"fixed_code\": \"def plot(result_pickle_file_path, show, plot_save_file):\\n    \\\"\\\"\\\"\\n    [sys_analyser] draw result DataFrame\\n    \\\"\\\"\\\"\\n    import pandas as pd\\n    from .plot import plot_result\\n\\n    result_dict = pd.read_pickle(result_pickle_file_path)\\n    plot_result(result_dict, show, plot_save_file)\",\n",
      "    \"ast_diff\": [\n",
      "        \"--- \",\n",
      "        \"+++ \",\n",
      "        \"@@ -1,23 +1,23 @@\",\n",
      "        \"-Module(body=[FunctionDef(name='plot', args=arguments(posonlyargs=[], args=[arg(arg='result_dict_file'), arg(arg='show'), arg(arg='plot_save_file')], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value='\\\\n    [sys_analyser] draw result DataFrame\\\\n    ')), Import(names=[alias(name='pandas', asname='pd')]), ImportFrom(module='plot', names=[alias(name='plot_result')], level=1), Assign(targets=[Name(id='result_dict', ctx=Store())], value=Call(func=Attribute(value=Name(id='pd', ctx=Load()), attr='read_pickle', ctx=Load()), args=[Name(id='result_dict_file', ctx=Load())], keywords=[])), Expr(value=Call(func=Name(id='plot_result', ctx=Load()), args=[Name(id='result_dict', ctx=Load()), Name(id='show', ctx=Load()), Name(id='plot_save_file', ctx=Load())], keywords=[]))], decorator_list=[], type_params=[])], type_ignores=[])\",\n",
      "        \"-FunctionDef(name='plot', args=arguments(posonlyargs=[], args=[arg(arg='result_dict_file'), arg(arg='show'), arg(arg='plot_save_file')], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value='\\\\n    [sys_analyser] draw result DataFrame\\\\n    ')), Import(names=[alias(name='pandas', asname='pd')]), ImportFrom(module='plot', names=[alias(name='plot_result')], level=1), Assign(targets=[Name(id='result_dict', ctx=Store())], value=Call(func=Attribute(value=Name(id='pd', ctx=Load()), attr='read_pickle', ctx=Load()), args=[Name(id='result_dict_file', ctx=Load())], keywords=[])), Expr(value=Call(func=Name(id='plot_result', ctx=Load()), args=[Name(id='result_dict', ctx=Load()), Name(id='show', ctx=Load()), Name(id='plot_save_file', ctx=Load())], keywords=[]))], decorator_list=[], type_params=[])\",\n",
      "        \"-arguments(posonlyargs=[], args=[arg(arg='result_dict_file'), arg(arg='show'), arg(arg='plot_save_file')], kwonlyargs=[], kw_defaults=[], defaults=[])\",\n",
      "        \"+Module(body=[FunctionDef(name='plot', args=arguments(posonlyargs=[], args=[arg(arg='result_pickle_file_path'), arg(arg='show'), arg(arg='plot_save_file')], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value='\\\\n    [sys_analyser] draw result DataFrame\\\\n    ')), Import(names=[alias(name='pandas', asname='pd')]), ImportFrom(module='plot', names=[alias(name='plot_result')], level=1), Assign(targets=[Name(id='result_dict', ctx=Store())], value=Call(func=Attribute(value=Name(id='pd', ctx=Load()), attr='read_pickle', ctx=Load()), args=[Name(id='result_pickle_file_path', ctx=Load())], keywords=[])), Expr(value=Call(func=Name(id='plot_result', ctx=Load()), args=[Name(id='result_dict', ctx=Load()), Name(id='show', ctx=Load()), Name(id='plot_save_file', ctx=Load())], keywords=[]))], decorator_list=[], type_params=[])], type_ignores=[])\",\n",
      "        \"+FunctionDef(name='plot', args=arguments(posonlyargs=[], args=[arg(arg='result_pickle_file_path'), arg(arg='show'), arg(arg='plot_save_file')], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value='\\\\n    [sys_analyser] draw result DataFrame\\\\n    ')), Import(names=[alias(name='pandas', asname='pd')]), ImportFrom(module='plot', names=[alias(name='plot_result')], level=1), Assign(targets=[Name(id='result_dict', ctx=Store())], value=Call(func=Attribute(value=Name(id='pd', ctx=Load()), attr='read_pickle', ctx=Load()), args=[Name(id='result_pickle_file_path', ctx=Load())], keywords=[])), Expr(value=Call(func=Name(id='plot_result', ctx=Load()), args=[Name(id='result_dict', ctx=Load()), Name(id='show', ctx=Load()), Name(id='plot_save_file', ctx=Load())], keywords=[]))], decorator_list=[], type_params=[])\",\n",
      "        \"+arguments(posonlyargs=[], args=[arg(arg='result_pickle_file_path'), arg(arg='show'), arg(arg='plot_save_file')], kwonlyargs=[], kw_defaults=[], defaults=[])\",\n",
      "        \" Expr(value=Constant(value='\\\\n    [sys_analyser] draw result DataFrame\\\\n    '))\",\n",
      "        \" Import(names=[alias(name='pandas', asname='pd')])\",\n",
      "        \" ImportFrom(module='plot', names=[alias(name='plot_result')], level=1)\",\n",
      "        \"-Assign(targets=[Name(id='result_dict', ctx=Store())], value=Call(func=Attribute(value=Name(id='pd', ctx=Load()), attr='read_pickle', ctx=Load()), args=[Name(id='result_dict_file', ctx=Load())], keywords=[]))\",\n",
      "        \"+Assign(targets=[Name(id='result_dict', ctx=Store())], value=Call(func=Attribute(value=Name(id='pd', ctx=Load()), attr='read_pickle', ctx=Load()), args=[Name(id='result_pickle_file_path', ctx=Load())], keywords=[]))\",\n",
      "        \" Expr(value=Call(func=Name(id='plot_result', ctx=Load()), args=[Name(id='result_dict', ctx=Load()), Name(id='show', ctx=Load()), Name(id='plot_save_file', ctx=Load())], keywords=[]))\",\n",
      "        \"-arg(arg='result_dict_file')\",\n",
      "        \"+arg(arg='result_pickle_file_path')\",\n",
      "        \" arg(arg='show')\",\n",
      "        \" arg(arg='plot_save_file')\",\n",
      "        \" Constant(value='\\\\n    [sys_analyser] draw result DataFrame\\\\n    ')\",\n",
      "        \" alias(name='pandas', asname='pd')\",\n",
      "        \" alias(name='plot_result')\",\n",
      "        \" Name(id='result_dict', ctx=Store())\",\n",
      "        \"-Call(func=Attribute(value=Name(id='pd', ctx=Load()), attr='read_pickle', ctx=Load()), args=[Name(id='result_dict_file', ctx=Load())], keywords=[])\",\n",
      "        \"+Call(func=Attribute(value=Name(id='pd', ctx=Load()), attr='read_pickle', ctx=Load()), args=[Name(id='result_pickle_file_path', ctx=Load())], keywords=[])\",\n",
      "        \" Call(func=Name(id='plot_result', ctx=Load()), args=[Name(id='result_dict', ctx=Load()), Name(id='show', ctx=Load()), Name(id='plot_save_file', ctx=Load())], keywords=[])\",\n",
      "        \" Store()\",\n",
      "        \" Attribute(value=Name(id='pd', ctx=Load()), attr='read_pickle', ctx=Load())\",\n",
      "        \"-Name(id='result_dict_file', ctx=Load())\",\n",
      "        \"+Name(id='result_pickle_file_path', ctx=Load())\",\n",
      "        \" Name(id='plot_result', ctx=Load())\",\n",
      "        \" Name(id='result_dict', ctx=Load())\",\n",
      "        \" Name(id='show', ctx=Load())\"\n",
      "    ],\n",
      "    \"traceback_type\": \"TypeError\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "formatted_diff = generate_ast_diff_json(code_before, code_after)\n",
    "print(formatted_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_training_example(entry):\n",
    "    prompt = (\n",
    "        f\"<s><INST>Below is a piece of buggy code along with additional context. Predict the type of traceback (error) that would occur if this code were executed.\\n\\n\"\n",
    "        f\"**Buggy Code:**\\n```\\n{entry['buggy_code']}\\n```\\n\\n\"\n",
    "        f\"**AST Diff:**\\n```\\n{chr(10).join(entry['ast_diff'])}\\n```\\n\\n\"\n",
    "        f\"**Task:**\\nProvide the traceback type.</INST>\\n\\n\"\n",
    "        f\"**Traceback Type:**\\n{entry['traceback_type']}</s>\"\n",
    "    )\n",
    "    return {\"text\": prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '<s><INST>Below is a piece of buggy code along with additional context. Predict the type of traceback (error) that would occur if this code were executed.\\n\\n**Buggy Code:**\\n```\\ndef plot(result_dict_file, show, plot_save_file):\\n    \"\"\"\\n    [sys_analyser] draw result DataFrame\\n    \"\"\"\\n    import pandas as pd\\n    from .plot import plot_result\\n\\n    result_dict = pd.read_pickle(result_dict_file)\\n    plot_result(result_dict, show, plot_save_file)\\n```\\n\\n**AST Diff:**\\n```\\n--- \\n+++ \\n@@ -1,23 +1,23 @@\\n-Module(body=[FunctionDef(name=\\'plot\\', args=arguments(posonlyargs=[], args=[arg(arg=\\'result_dict_file\\'), arg(arg=\\'show\\'), arg(arg=\\'plot_save_file\\')], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=\\'\\\\n    [sys_analyser] draw result DataFrame\\\\n    \\')), Import(names=[alias(name=\\'pandas\\', asname=\\'pd\\')]), ImportFrom(module=\\'plot\\', names=[alias(name=\\'plot_result\\')], level=1), Assign(targets=[Name(id=\\'result_dict\\', ctx=Store())], value=Call(func=Attribute(value=Name(id=\\'pd\\', ctx=Load()), attr=\\'read_pickle\\', ctx=Load()), args=[Name(id=\\'result_dict_file\\', ctx=Load())], keywords=[])), Expr(value=Call(func=Name(id=\\'plot_result\\', ctx=Load()), args=[Name(id=\\'result_dict\\', ctx=Load()), Name(id=\\'show\\', ctx=Load()), Name(id=\\'plot_save_file\\', ctx=Load())], keywords=[]))], decorator_list=[], type_params=[])], type_ignores=[])\\n-FunctionDef(name=\\'plot\\', args=arguments(posonlyargs=[], args=[arg(arg=\\'result_dict_file\\'), arg(arg=\\'show\\'), arg(arg=\\'plot_save_file\\')], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=\\'\\\\n    [sys_analyser] draw result DataFrame\\\\n    \\')), Import(names=[alias(name=\\'pandas\\', asname=\\'pd\\')]), ImportFrom(module=\\'plot\\', names=[alias(name=\\'plot_result\\')], level=1), Assign(targets=[Name(id=\\'result_dict\\', ctx=Store())], value=Call(func=Attribute(value=Name(id=\\'pd\\', ctx=Load()), attr=\\'read_pickle\\', ctx=Load()), args=[Name(id=\\'result_dict_file\\', ctx=Load())], keywords=[])), Expr(value=Call(func=Name(id=\\'plot_result\\', ctx=Load()), args=[Name(id=\\'result_dict\\', ctx=Load()), Name(id=\\'show\\', ctx=Load()), Name(id=\\'plot_save_file\\', ctx=Load())], keywords=[]))], decorator_list=[], type_params=[])\\n-arguments(posonlyargs=[], args=[arg(arg=\\'result_dict_file\\'), arg(arg=\\'show\\'), arg(arg=\\'plot_save_file\\')], kwonlyargs=[], kw_defaults=[], defaults=[])\\n+Module(body=[FunctionDef(name=\\'plot\\', args=arguments(posonlyargs=[], args=[arg(arg=\\'result_pickle_file_path\\'), arg(arg=\\'show\\'), arg(arg=\\'plot_save_file\\')], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=\\'\\\\n    [sys_analyser] draw result DataFrame\\\\n    \\')), Import(names=[alias(name=\\'pandas\\', asname=\\'pd\\')]), ImportFrom(module=\\'plot\\', names=[alias(name=\\'plot_result\\')], level=1), Assign(targets=[Name(id=\\'result_dict\\', ctx=Store())], value=Call(func=Attribute(value=Name(id=\\'pd\\', ctx=Load()), attr=\\'read_pickle\\', ctx=Load()), args=[Name(id=\\'result_pickle_file_path\\', ctx=Load())], keywords=[])), Expr(value=Call(func=Name(id=\\'plot_result\\', ctx=Load()), args=[Name(id=\\'result_dict\\', ctx=Load()), Name(id=\\'show\\', ctx=Load()), Name(id=\\'plot_save_file\\', ctx=Load())], keywords=[]))], decorator_list=[], type_params=[])], type_ignores=[])\\n+FunctionDef(name=\\'plot\\', args=arguments(posonlyargs=[], args=[arg(arg=\\'result_pickle_file_path\\'), arg(arg=\\'show\\'), arg(arg=\\'plot_save_file\\')], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=\\'\\\\n    [sys_analyser] draw result DataFrame\\\\n    \\')), Import(names=[alias(name=\\'pandas\\', asname=\\'pd\\')]), ImportFrom(module=\\'plot\\', names=[alias(name=\\'plot_result\\')], level=1), Assign(targets=[Name(id=\\'result_dict\\', ctx=Store())], value=Call(func=Attribute(value=Name(id=\\'pd\\', ctx=Load()), attr=\\'read_pickle\\', ctx=Load()), args=[Name(id=\\'result_pickle_file_path\\', ctx=Load())], keywords=[])), Expr(value=Call(func=Name(id=\\'plot_result\\', ctx=Load()), args=[Name(id=\\'result_dict\\', ctx=Load()), Name(id=\\'show\\', ctx=Load()), Name(id=\\'plot_save_file\\', ctx=Load())], keywords=[]))], decorator_list=[], type_params=[])\\n+arguments(posonlyargs=[], args=[arg(arg=\\'result_pickle_file_path\\'), arg(arg=\\'show\\'), arg(arg=\\'plot_save_file\\')], kwonlyargs=[], kw_defaults=[], defaults=[])\\n Expr(value=Constant(value=\\'\\\\n    [sys_analyser] draw result DataFrame\\\\n    \\'))\\n Import(names=[alias(name=\\'pandas\\', asname=\\'pd\\')])\\n ImportFrom(module=\\'plot\\', names=[alias(name=\\'plot_result\\')], level=1)\\n-Assign(targets=[Name(id=\\'result_dict\\', ctx=Store())], value=Call(func=Attribute(value=Name(id=\\'pd\\', ctx=Load()), attr=\\'read_pickle\\', ctx=Load()), args=[Name(id=\\'result_dict_file\\', ctx=Load())], keywords=[]))\\n+Assign(targets=[Name(id=\\'result_dict\\', ctx=Store())], value=Call(func=Attribute(value=Name(id=\\'pd\\', ctx=Load()), attr=\\'read_pickle\\', ctx=Load()), args=[Name(id=\\'result_pickle_file_path\\', ctx=Load())], keywords=[]))\\n Expr(value=Call(func=Name(id=\\'plot_result\\', ctx=Load()), args=[Name(id=\\'result_dict\\', ctx=Load()), Name(id=\\'show\\', ctx=Load()), Name(id=\\'plot_save_file\\', ctx=Load())], keywords=[]))\\n-arg(arg=\\'result_dict_file\\')\\n+arg(arg=\\'result_pickle_file_path\\')\\n arg(arg=\\'show\\')\\n arg(arg=\\'plot_save_file\\')\\n Constant(value=\\'\\\\n    [sys_analyser] draw result DataFrame\\\\n    \\')\\n alias(name=\\'pandas\\', asname=\\'pd\\')\\n alias(name=\\'plot_result\\')\\n Name(id=\\'result_dict\\', ctx=Store())\\n-Call(func=Attribute(value=Name(id=\\'pd\\', ctx=Load()), attr=\\'read_pickle\\', ctx=Load()), args=[Name(id=\\'result_dict_file\\', ctx=Load())], keywords=[])\\n+Call(func=Attribute(value=Name(id=\\'pd\\', ctx=Load()), attr=\\'read_pickle\\', ctx=Load()), args=[Name(id=\\'result_pickle_file_path\\', ctx=Load())], keywords=[])\\n Call(func=Name(id=\\'plot_result\\', ctx=Load()), args=[Name(id=\\'result_dict\\', ctx=Load()), Name(id=\\'show\\', ctx=Load()), Name(id=\\'plot_save_file\\', ctx=Load())], keywords=[])\\n Store()\\n Attribute(value=Name(id=\\'pd\\', ctx=Load()), attr=\\'read_pickle\\', ctx=Load())\\n-Name(id=\\'result_dict_file\\', ctx=Load())\\n+Name(id=\\'result_pickle_file_path\\', ctx=Load())\\n Name(id=\\'plot_result\\', ctx=Load())\\n Name(id=\\'result_dict\\', ctx=Load())\\n Name(id=\\'show\\', ctx=Load())\\n```\\n\\n**Task:**\\nProvide the traceback type.</INST>\\n\\n**Traceback Type:**\\nTypeError</s>'}\n"
     ]
    }
   ],
   "source": [
    "print(format_training_example(json.loads(formatted_diff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After fine-tuning, test the model with prompts like <s><INST>Fix this buggy code: ... </INST> and check if it outputs the correct fixed_code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dummy = pd.read_pickle('buggy_dataset/bugfixes_train.pickle')\n",
    "val_data_dummy = pd.read_pickle('buggy_dataset/bugfixes_valid.pickle')\n",
    "test_data_dummy = pd.read_pickle('buggy_dataset/bugfixes_test.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14118, 16)\n",
      "(9457, 16)\n",
      "(161, 12)\n"
     ]
    }
   ],
   "source": [
    "# print(train_data_dummy['full_traceback'].value_counts().head(10))\n",
    "# print('*'*20)\n",
    "# print(val_data_dummy['full_traceback'].value_counts().head(10))\n",
    "# print('*'*20)\n",
    "# print(test_data_dummy['full_traceback'].value_counts().head(10))\n",
    "# print('*'*20)\n",
    "\n",
    "# get number of entries in each data dummy set\n",
    "print(train_data_dummy.shape)\n",
    "print(val_data_dummy.shape)\n",
    "print(test_data_dummy.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traceback_type\n",
      "31mAssertionError                               2\n",
      "3241TypeError                                   2\n",
      "AATypeError                                     2\n",
      "APIError                                        2\n",
      "APIException                                    5\n",
      "                                               ..\n",
      "yt.utilities.exceptions.YTUfuncUnitError        1\n",
      "yt.utilities.exceptions.YTUnitOperationError    1\n",
      "zerorpc.exceptions.RemoteError                  8\n",
      "zlib.error                                      2\n",
      "zmq.ZMQError                                    2\n",
      "Name: before_merge, Length: 555, dtype: int64\n",
      "traceback_type\n",
      "31mAssertionError                               2\n",
      "3241TypeError                                   2\n",
      "AATypeError                                     2\n",
      "APIError                                        2\n",
      "APIException                                    5\n",
      "                                               ..\n",
      "yt.utilities.exceptions.YTUfuncUnitError        1\n",
      "yt.utilities.exceptions.YTUnitOperationError    1\n",
      "zerorpc.exceptions.RemoteError                  8\n",
      "zlib.error                                      2\n",
      "zmq.ZMQError                                    2\n",
      "Name: after_merge, Length: 555, dtype: int64\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "df77 = train_data_dummy[[\"before_merge\", \"after_merge\", \"traceback_type\", \"full_traceback\", \"source code and errors\", \"function_name\", \"before_merge_docstrings\", \"after_merge_docstrings\"]]\n",
    "# How many of the entries are in each traceback type\n",
    "# print(df77['traceback_type'].value_counts())\n",
    "# print(df77['traceback_type'].value_counts(normalize=True))\n",
    "\n",
    "# are the number of before and after merge entries the same for each full traceback type?\n",
    "# print(df77.groupby('full_traceback')['before_merge'].count())\n",
    "# print(df77.groupby('full_traceback')['after_merge'].count())\n",
    "\n",
    "# are the number of before and after merge entries the same for each traceback type?\n",
    "a = df77.groupby('traceback_type')['before_merge'].count()\n",
    "b = df77.groupby('traceback_type')['after_merge'].count()\n",
    "\n",
    "# are a and b giving the same results?\n",
    "print(a)\n",
    "print(b)\n",
    "print(a.equals(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m MODEL_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcodellama/CodeLlama-7b-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Can be replaced with a suitable base model\u001b[39;00m\n\u001b[1;32m     25\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_NAME)\n\u001b[0;32m---> 26\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_NAME, load_in_8bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Define LoRA configuration\u001b[39;00m\n\u001b[1;32m     29\u001b[0m lora_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m     30\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, target_modules\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m], task_type\u001b[38;5;241m=\u001b[39mTaskType\u001b[38;5;241m.\u001b[39mCAUSAL_LM\n\u001b[1;32m     31\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    566\u001b[0m     )\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:262\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:3698\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3695\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3698\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mvalidate_environment(\n\u001b[1;32m   3699\u001b[0m         torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype,\n\u001b[1;32m   3700\u001b[0m         from_tf\u001b[38;5;241m=\u001b[39mfrom_tf,\n\u001b[1;32m   3701\u001b[0m         from_flax\u001b[38;5;241m=\u001b[39mfrom_flax,\n\u001b[1;32m   3702\u001b[0m         device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[1;32m   3703\u001b[0m         weights_only\u001b[38;5;241m=\u001b[39mweights_only,\n\u001b[1;32m   3704\u001b[0m     )\n\u001b[1;32m   3705\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3706\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_8bit.py:73\u001b[0m, in \u001b[0;36mBnb8BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m     )\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bitsandbytes_available():\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_bnb_backend_availability\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n",
      "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "\n",
    "# Load dataset\n",
    "# Assuming PyTraceBugs dataset is preprocessed into train, validation, and test pickle files\n",
    "import pandas as pd\n",
    "train_data = pd.read_pickle('buggy_dataset/bugfixes_train.pickle')\n",
    "val_data = pd.read_pickle('buggy_dataset/bugfixes_valid.pickle')\n",
    "test_data = pd.read_pickle('buggy_dataset/bugfixes_test.pickle')\n",
    "\n",
    "# Extract relevant columns\n",
    "train_texts = train_data['before_merge'] + \"\\n\" + train_data['full_traceback']\n",
    "train_labels = train_data['after_merge']\n",
    "val_texts = val_data['before_merge'] + \"\\n\" + val_data['full_traceback']\n",
    "val_labels = val_data['after_merge']\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Load Tokenizer and Model\n",
    "MODEL_NAME = \"codellama/CodeLlama-7b-hf\"  # Can be replaced with a suitable base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, load_in_8bit=True, device_map=\"auto\")\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8, alpha=16, target_modules=[\"q_proj\", \"v_proj\"], task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Tokenize dataset\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"train.csv\", \"validation\": \"val.csv\"})\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine_tuned_llm\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_llm\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
